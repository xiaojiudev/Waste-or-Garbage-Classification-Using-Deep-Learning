import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import ResNet50
from tensorflow.keras import layers, models, optimizers

# Kiá»ƒm tra GPU
print("GPU available:", tf.config.list_physical_devices('GPU'))
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    tf.config.set_logical_device_configuration(
        gpus[0],
        [tf.config.LogicalDeviceConfiguration(memory_limit=8192)]
    )

# ÄÆ°á»ng dáº«n dá»¯ liá»‡u
train_path = 'D:/Study/CNTT/A.MHUD/CNN_Practice/train'
test_path = 'D:/Study/CNTT/A.MHUD/CNN_Practice/test'

# ğŸ”¥ Cáº£i thiá»‡n Data Augmentation
train_datagen = ImageDataGenerator(
    rescale=1.0/255,
    rotation_range=30,       # TÄƒng Ä‘á»™ xoay lÃªn 30 Ä‘á»™
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    brightness_range=[0.5, 1.5],  # Äiá»u chá»‰nh Ä‘á»™ sÃ¡ng
    fill_mode='nearest'
)

test_datagen = ImageDataGenerator(rescale=1.0/255)

# Load dataset
train_generator = train_datagen.flow_from_directory(
    train_path, target_size=(224, 224), batch_size=32, class_mode='categorical')

test_generator = test_datagen.flow_from_directory(
    test_path, target_size=(224, 224), batch_size=32, class_mode='categorical')

# ğŸ”¥ Load ResNet50 (Giai Ä‘oáº¡n 1: Freeze toÃ n bá»™)
base_model = ResNet50(input_shape=(224, 224, 3), weights='imagenet', include_top=False)
base_model.trainable = False  # Ban Ä‘áº§u Ä‘Ã³ng bÄƒng toÃ n bá»™ máº¡ng ResNet50

# XÃ¢y dá»±ng mÃ´ hÃ¬nh
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(512, activation='relu')(x)
x = layers.BatchNormalization()(x)  # ğŸ”¥ ThÃªm BatchNorm
x = layers.Dropout(0.6)(x)  # ğŸ”¥ TÄƒng Dropout lÃªn 0.6
x = layers.Dense(7, activation='softmax')(x)

model = models.Model(inputs=base_model.input, outputs=x)

# ğŸ”¥ Sá»­ dá»¥ng Cosine Decay Learning Rate
lr_schedule = optimizers.schedules.CosineDecay(initial_learning_rate=1e-3, decay_steps=10000, alpha=1e-6)
optimizer = optimizers.Adam(learning_rate=lr_schedule)

model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Callbacks
checkpoint = ModelCheckpoint('resnet50_best.h5', save_best_only=True, monitor='val_loss', mode='min')
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# ğŸ”¥ Huáº¥n luyá»‡n giai Ä‘oáº¡n 1 (Chá»‰ train FC Layer)
history = model.fit(
    train_generator, epochs=15, validation_data=test_generator, callbacks=[checkpoint, early_stop]
)

# ğŸ”¥ Giai Ä‘oáº¡n 2: Fine-tune 50 lá»›p cuá»‘i
base_model.trainable = True
for layer in base_model.layers[:-50]:  # ğŸ”¥ Chá»‰ má»Ÿ khÃ³a 50 lá»›p cuá»‘i
    layer.trainable = False

# Compile láº¡i vá»›i learning rate tháº¥p hÆ¡n
model.compile(optimizer=optimizers.Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])

# ğŸ”¥ Huáº¥n luyá»‡n láº¡i vá»›i Fine-Tuning
history_finetune = model.fit(
    train_generator, epochs=30, validation_data=test_generator, callbacks=[checkpoint, early_stop]
)

# ğŸ”¥ ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.plot(history_finetune.history['accuracy'], label='Fine-Tuned Train Accuracy')
plt.plot(history_finetune.history['val_accuracy'], label='Fine-Tuned Val Accuracy')
plt.legend()
plt.show()

# Kiá»ƒm tra trÃªn táº­p test
test_loss, test_acc = model.evaluate(test_generator)
print(f'Test Accuracy: {test_acc:.4f}')

# Dá»± Ä‘oÃ¡n vÃ  Ä‘Ã¡nh giÃ¡
y_pred = model.predict(test_generator)
y_pred = np.argmax(y_pred, axis=1)
y_true = test_generator.classes

from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(y_true, y_pred, target_names=test_generator.class_indices.keys()))
print(confusion_matrix(y_true, y_pred))
