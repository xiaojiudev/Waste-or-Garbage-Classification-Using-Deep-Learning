27-03-2025-17-06

"C:\Program Files\Python310\python.exe" D:\Study\CNTT\A.MHUD\CNN_Practice\MODEL_EFFICIENTNET.py
GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
TensorFlow is using GPU: True
2025-03-27 17:08:03.418218: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-27 17:08:03.885678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8192 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6
1 Physical GPU, 1 Logical GPUs
Found 10321 images belonging to 9 classes.
Found 2574 images belonging to 9 classes.
Found 5521 images belonging to 9 classes.
Training class distribution: Counter({2: 2820, 3: 1124, 7: 1112, 8: 1108, 5: 1020, 6: 868, 1: 779, 4: 762, 0: 728})
Validation class distribution: Counter({2: 704, 3: 281, 7: 277, 8: 276, 5: 254, 6: 216, 1: 194, 4: 190, 0: 182})
Testing class distribution: Counter({2: 1509, 3: 602, 7: 594, 8: 592, 5: 545, 6: 464, 1: 417, 4: 408, 0: 390})
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to
==================================================================================================
 input_1 (InputLayer)           [(None, 224, 224, 3  0           []
                                )]

 rescaling (Rescaling)          (None, 224, 224, 3)  0           ['input_1[0][0]']

 normalization (Normalization)  (None, 224, 224, 3)  7           ['rescaling[0][0]']

 tf.math.truediv (TFOpLambda)   (None, 224, 224, 3)  0           ['normalization[0][0]']

 stem_conv_pad (ZeroPadding2D)  (None, 225, 225, 3)  0           ['tf.math.truediv[0][0]']

 stem_conv (Conv2D)             (None, 112, 112, 32  864         ['stem_conv_pad[0][0]']
                                )

 stem_bn (BatchNormalization)   (None, 112, 112, 32  128         ['stem_conv[0][0]']
                                )

 stem_activation (Activation)   (None, 112, 112, 32  0           ['stem_bn[0][0]']
                                )

 block1a_dwconv (DepthwiseConv2  (None, 112, 112, 32  288        ['stem_activation[0][0]']
 D)                             )

 block1a_bn (BatchNormalization  (None, 112, 112, 32  128        ['block1a_dwconv[0][0]']
 )                              )

 block1a_activation (Activation  (None, 112, 112, 32  0          ['block1a_bn[0][0]']
 )                              )

 block1a_se_squeeze (GlobalAver  (None, 32)          0           ['block1a_activation[0][0]']
 agePooling2D)

 block1a_se_reshape (Reshape)   (None, 1, 1, 32)     0           ['block1a_se_squeeze[0][0]']

 block1a_se_reduce (Conv2D)     (None, 1, 1, 8)      264         ['block1a_se_reshape[0][0]']

 block1a_se_expand (Conv2D)     (None, 1, 1, 32)     288         ['block1a_se_reduce[0][0]']

 block1a_se_excite (Multiply)   (None, 112, 112, 32  0           ['block1a_activation[0][0]',
                                )                                 'block1a_se_expand[0][0]']

 block1a_project_conv (Conv2D)  (None, 112, 112, 16  512         ['block1a_se_excite[0][0]']
                                )

 block1a_project_bn (BatchNorma  (None, 112, 112, 16  64         ['block1a_project_conv[0][0]']
 lization)                      )

 block2a_expand_conv (Conv2D)   (None, 112, 112, 96  1536        ['block1a_project_bn[0][0]']
                                )

 block2a_expand_bn (BatchNormal  (None, 112, 112, 96  384        ['block2a_expand_conv[0][0]']
 ization)                       )

 block2a_expand_activation (Act  (None, 112, 112, 96  0          ['block2a_expand_bn[0][0]']
 ivation)                       )

 block2a_dwconv_pad (ZeroPaddin  (None, 113, 113, 96  0          ['block2a_expand_activation[0][0]
 g2D)                           )                                ']

 block2a_dwconv (DepthwiseConv2  (None, 56, 56, 96)  864         ['block2a_dwconv_pad[0][0]']
 D)

 block2a_bn (BatchNormalization  (None, 56, 56, 96)  384         ['block2a_dwconv[0][0]']
 )

 block2a_activation (Activation  (None, 56, 56, 96)  0           ['block2a_bn[0][0]']
 )

 block2a_se_squeeze (GlobalAver  (None, 96)          0           ['block2a_activation[0][0]']
 agePooling2D)

 block2a_se_reshape (Reshape)   (None, 1, 1, 96)     0           ['block2a_se_squeeze[0][0]']

 block2a_se_reduce (Conv2D)     (None, 1, 1, 4)      388         ['block2a_se_reshape[0][0]']

 block2a_se_expand (Conv2D)     (None, 1, 1, 96)     480         ['block2a_se_reduce[0][0]']

 block2a_se_excite (Multiply)   (None, 56, 56, 96)   0           ['block2a_activation[0][0]',
                                                                  'block2a_se_expand[0][0]']

 block2a_project_conv (Conv2D)  (None, 56, 56, 24)   2304        ['block2a_se_excite[0][0]']

 block2a_project_bn (BatchNorma  (None, 56, 56, 24)  96          ['block2a_project_conv[0][0]']
 lization)

 block2b_expand_conv (Conv2D)   (None, 56, 56, 144)  3456        ['block2a_project_bn[0][0]']

 block2b_expand_bn (BatchNormal  (None, 56, 56, 144)  576        ['block2b_expand_conv[0][0]']
 ization)

 block2b_expand_activation (Act  (None, 56, 56, 144)  0          ['block2b_expand_bn[0][0]']
 ivation)

 block2b_dwconv (DepthwiseConv2  (None, 56, 56, 144)  1296       ['block2b_expand_activation[0][0]
 D)                                                              ']

 block2b_bn (BatchNormalization  (None, 56, 56, 144)  576        ['block2b_dwconv[0][0]']
 )

 block2b_activation (Activation  (None, 56, 56, 144)  0          ['block2b_bn[0][0]']
 )

 block2b_se_squeeze (GlobalAver  (None, 144)         0           ['block2b_activation[0][0]']
 agePooling2D)

 block2b_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block2b_se_squeeze[0][0]']

 block2b_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block2b_se_reshape[0][0]']

 block2b_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block2b_se_reduce[0][0]']

 block2b_se_excite (Multiply)   (None, 56, 56, 144)  0           ['block2b_activation[0][0]',
                                                                  'block2b_se_expand[0][0]']

 block2b_project_conv (Conv2D)  (None, 56, 56, 24)   3456        ['block2b_se_excite[0][0]']

 block2b_project_bn (BatchNorma  (None, 56, 56, 24)  96          ['block2b_project_conv[0][0]']
 lization)

 block2b_drop (Dropout)         (None, 56, 56, 24)   0           ['block2b_project_bn[0][0]']

 block2b_add (Add)              (None, 56, 56, 24)   0           ['block2b_drop[0][0]',
                                                                  'block2a_project_bn[0][0]']

 block3a_expand_conv (Conv2D)   (None, 56, 56, 144)  3456        ['block2b_add[0][0]']

 block3a_expand_bn (BatchNormal  (None, 56, 56, 144)  576        ['block3a_expand_conv[0][0]']
 ization)

 block3a_expand_activation (Act  (None, 56, 56, 144)  0          ['block3a_expand_bn[0][0]']
 ivation)

 block3a_dwconv_pad (ZeroPaddin  (None, 59, 59, 144)  0          ['block3a_expand_activation[0][0]
 g2D)                                                            ']

 block3a_dwconv (DepthwiseConv2  (None, 28, 28, 144)  3600       ['block3a_dwconv_pad[0][0]']
 D)

 block3a_bn (BatchNormalization  (None, 28, 28, 144)  576        ['block3a_dwconv[0][0]']
 )

 block3a_activation (Activation  (None, 28, 28, 144)  0          ['block3a_bn[0][0]']
 )

 block3a_se_squeeze (GlobalAver  (None, 144)         0           ['block3a_activation[0][0]']
 agePooling2D)

 block3a_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block3a_se_squeeze[0][0]']

 block3a_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block3a_se_reshape[0][0]']

 block3a_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block3a_se_reduce[0][0]']

 block3a_se_excite (Multiply)   (None, 28, 28, 144)  0           ['block3a_activation[0][0]',
                                                                  'block3a_se_expand[0][0]']

 block3a_project_conv (Conv2D)  (None, 28, 28, 40)   5760        ['block3a_se_excite[0][0]']

 block3a_project_bn (BatchNorma  (None, 28, 28, 40)  160         ['block3a_project_conv[0][0]']
 lization)

 block3b_expand_conv (Conv2D)   (None, 28, 28, 240)  9600        ['block3a_project_bn[0][0]']

 block3b_expand_bn (BatchNormal  (None, 28, 28, 240)  960        ['block3b_expand_conv[0][0]']
 ization)

 block3b_expand_activation (Act  (None, 28, 28, 240)  0          ['block3b_expand_bn[0][0]']
 ivation)

 block3b_dwconv (DepthwiseConv2  (None, 28, 28, 240)  6000       ['block3b_expand_activation[0][0]
 D)                                                              ']

 block3b_bn (BatchNormalization  (None, 28, 28, 240)  960        ['block3b_dwconv[0][0]']
 )

 block3b_activation (Activation  (None, 28, 28, 240)  0          ['block3b_bn[0][0]']
 )

 block3b_se_squeeze (GlobalAver  (None, 240)         0           ['block3b_activation[0][0]']
 agePooling2D)

 block3b_se_reshape (Reshape)   (None, 1, 1, 240)    0           ['block3b_se_squeeze[0][0]']

 block3b_se_reduce (Conv2D)     (None, 1, 1, 10)     2410        ['block3b_se_reshape[0][0]']

 block3b_se_expand (Conv2D)     (None, 1, 1, 240)    2640        ['block3b_se_reduce[0][0]']

 block3b_se_excite (Multiply)   (None, 28, 28, 240)  0           ['block3b_activation[0][0]',
                                                                  'block3b_se_expand[0][0]']

 block3b_project_conv (Conv2D)  (None, 28, 28, 40)   9600        ['block3b_se_excite[0][0]']

 block3b_project_bn (BatchNorma  (None, 28, 28, 40)  160         ['block3b_project_conv[0][0]']
 lization)

 block3b_drop (Dropout)         (None, 28, 28, 40)   0           ['block3b_project_bn[0][0]']

 block3b_add (Add)              (None, 28, 28, 40)   0           ['block3b_drop[0][0]',
                                                                  'block3a_project_bn[0][0]']

 block4a_expand_conv (Conv2D)   (None, 28, 28, 240)  9600        ['block3b_add[0][0]']

 block4a_expand_bn (BatchNormal  (None, 28, 28, 240)  960        ['block4a_expand_conv[0][0]']
 ization)

 block4a_expand_activation (Act  (None, 28, 28, 240)  0          ['block4a_expand_bn[0][0]']
 ivation)

 block4a_dwconv_pad (ZeroPaddin  (None, 29, 29, 240)  0          ['block4a_expand_activation[0][0]
 g2D)                                                            ']

 block4a_dwconv (DepthwiseConv2  (None, 14, 14, 240)  2160       ['block4a_dwconv_pad[0][0]']
 D)

 block4a_bn (BatchNormalization  (None, 14, 14, 240)  960        ['block4a_dwconv[0][0]']
 )

 block4a_activation (Activation  (None, 14, 14, 240)  0          ['block4a_bn[0][0]']
 )

 block4a_se_squeeze (GlobalAver  (None, 240)         0           ['block4a_activation[0][0]']
 agePooling2D)

 block4a_se_reshape (Reshape)   (None, 1, 1, 240)    0           ['block4a_se_squeeze[0][0]']

 block4a_se_reduce (Conv2D)     (None, 1, 1, 10)     2410        ['block4a_se_reshape[0][0]']

 block4a_se_expand (Conv2D)     (None, 1, 1, 240)    2640        ['block4a_se_reduce[0][0]']

 block4a_se_excite (Multiply)   (None, 14, 14, 240)  0           ['block4a_activation[0][0]',
                                                                  'block4a_se_expand[0][0]']

 block4a_project_conv (Conv2D)  (None, 14, 14, 80)   19200       ['block4a_se_excite[0][0]']

 block4a_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4a_project_conv[0][0]']
 lization)

 block4b_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4a_project_bn[0][0]']

 block4b_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block4b_expand_conv[0][0]']
 ization)

 block4b_expand_activation (Act  (None, 14, 14, 480)  0          ['block4b_expand_bn[0][0]']
 ivation)

 block4b_dwconv (DepthwiseConv2  (None, 14, 14, 480)  4320       ['block4b_expand_activation[0][0]
 D)                                                              ']

 block4b_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block4b_dwconv[0][0]']
 )

 block4b_activation (Activation  (None, 14, 14, 480)  0          ['block4b_bn[0][0]']
 )

 block4b_se_squeeze (GlobalAver  (None, 480)         0           ['block4b_activation[0][0]']
 agePooling2D)

 block4b_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block4b_se_squeeze[0][0]']

 block4b_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block4b_se_reshape[0][0]']

 block4b_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block4b_se_reduce[0][0]']

 block4b_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block4b_activation[0][0]',
                                                                  'block4b_se_expand[0][0]']

 block4b_project_conv (Conv2D)  (None, 14, 14, 80)   38400       ['block4b_se_excite[0][0]']

 block4b_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4b_project_conv[0][0]']
 lization)

 block4b_drop (Dropout)         (None, 14, 14, 80)   0           ['block4b_project_bn[0][0]']

 block4b_add (Add)              (None, 14, 14, 80)   0           ['block4b_drop[0][0]',
                                                                  'block4a_project_bn[0][0]']

 block4c_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4b_add[0][0]']

 block4c_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block4c_expand_conv[0][0]']
 ization)

 block4c_expand_activation (Act  (None, 14, 14, 480)  0          ['block4c_expand_bn[0][0]']
 ivation)

 block4c_dwconv (DepthwiseConv2  (None, 14, 14, 480)  4320       ['block4c_expand_activation[0][0]
 D)                                                              ']

 block4c_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block4c_dwconv[0][0]']
 )

 block4c_activation (Activation  (None, 14, 14, 480)  0          ['block4c_bn[0][0]']
 )

 block4c_se_squeeze (GlobalAver  (None, 480)         0           ['block4c_activation[0][0]']
 agePooling2D)

 block4c_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block4c_se_squeeze[0][0]']

 block4c_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block4c_se_reshape[0][0]']

 block4c_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block4c_se_reduce[0][0]']

 block4c_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block4c_activation[0][0]',
                                                                  'block4c_se_expand[0][0]']

 block4c_project_conv (Conv2D)  (None, 14, 14, 80)   38400       ['block4c_se_excite[0][0]']

 block4c_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4c_project_conv[0][0]']
 lization)

 block4c_drop (Dropout)         (None, 14, 14, 80)   0           ['block4c_project_bn[0][0]']

 block4c_add (Add)              (None, 14, 14, 80)   0           ['block4c_drop[0][0]',
                                                                  'block4b_add[0][0]']

 block5a_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4c_add[0][0]']

 block5a_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block5a_expand_conv[0][0]']
 ization)

 block5a_expand_activation (Act  (None, 14, 14, 480)  0          ['block5a_expand_bn[0][0]']
 ivation)

 block5a_dwconv (DepthwiseConv2  (None, 14, 14, 480)  12000      ['block5a_expand_activation[0][0]
 D)                                                              ']

 block5a_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block5a_dwconv[0][0]']
 )

 block5a_activation (Activation  (None, 14, 14, 480)  0          ['block5a_bn[0][0]']
 )

 block5a_se_squeeze (GlobalAver  (None, 480)         0           ['block5a_activation[0][0]']
 agePooling2D)

 block5a_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block5a_se_squeeze[0][0]']

 block5a_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block5a_se_reshape[0][0]']

 block5a_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block5a_se_reduce[0][0]']

 block5a_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block5a_activation[0][0]',
                                                                  'block5a_se_expand[0][0]']

 block5a_project_conv (Conv2D)  (None, 14, 14, 112)  53760       ['block5a_se_excite[0][0]']

 block5a_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5a_project_conv[0][0]']
 lization)

 block5b_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5a_project_bn[0][0]']

 block5b_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block5b_expand_conv[0][0]']
 ization)

 block5b_expand_activation (Act  (None, 14, 14, 672)  0          ['block5b_expand_bn[0][0]']
 ivation)

 block5b_dwconv (DepthwiseConv2  (None, 14, 14, 672)  16800      ['block5b_expand_activation[0][0]
 D)                                                              ']

 block5b_bn (BatchNormalization  (None, 14, 14, 672)  2688       ['block5b_dwconv[0][0]']
 )

 block5b_activation (Activation  (None, 14, 14, 672)  0          ['block5b_bn[0][0]']
 )

 block5b_se_squeeze (GlobalAver  (None, 672)         0           ['block5b_activation[0][0]']
 agePooling2D)

 block5b_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5b_se_squeeze[0][0]']

 block5b_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5b_se_reshape[0][0]']

 block5b_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5b_se_reduce[0][0]']

 block5b_se_excite (Multiply)   (None, 14, 14, 672)  0           ['block5b_activation[0][0]',
                                                                  'block5b_se_expand[0][0]']

 block5b_project_conv (Conv2D)  (None, 14, 14, 112)  75264       ['block5b_se_excite[0][0]']

 block5b_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5b_project_conv[0][0]']
 lization)

 block5b_drop (Dropout)         (None, 14, 14, 112)  0           ['block5b_project_bn[0][0]']

 block5b_add (Add)              (None, 14, 14, 112)  0           ['block5b_drop[0][0]',
                                                                  'block5a_project_bn[0][0]']

 block5c_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5b_add[0][0]']

 block5c_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block5c_expand_conv[0][0]']
 ization)

 block5c_expand_activation (Act  (None, 14, 14, 672)  0          ['block5c_expand_bn[0][0]']
 ivation)

 block5c_dwconv (DepthwiseConv2  (None, 14, 14, 672)  16800      ['block5c_expand_activation[0][0]
 D)                                                              ']

 block5c_bn (BatchNormalization  (None, 14, 14, 672)  2688       ['block5c_dwconv[0][0]']
 )

 block5c_activation (Activation  (None, 14, 14, 672)  0          ['block5c_bn[0][0]']
 )

 block5c_se_squeeze (GlobalAver  (None, 672)         0           ['block5c_activation[0][0]']
 agePooling2D)

 block5c_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5c_se_squeeze[0][0]']

 block5c_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5c_se_reshape[0][0]']

 block5c_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5c_se_reduce[0][0]']

 block5c_se_excite (Multiply)   (None, 14, 14, 672)  0           ['block5c_activation[0][0]',
                                                                  'block5c_se_expand[0][0]']

 block5c_project_conv (Conv2D)  (None, 14, 14, 112)  75264       ['block5c_se_excite[0][0]']

 block5c_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5c_project_conv[0][0]']
 lization)

 block5c_drop (Dropout)         (None, 14, 14, 112)  0           ['block5c_project_bn[0][0]']

 block5c_add (Add)              (None, 14, 14, 112)  0           ['block5c_drop[0][0]',
                                                                  'block5b_add[0][0]']

 block6a_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5c_add[0][0]']

 block6a_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block6a_expand_conv[0][0]']
 ization)

 block6a_expand_activation (Act  (None, 14, 14, 672)  0          ['block6a_expand_bn[0][0]']
 ivation)

 block6a_dwconv_pad (ZeroPaddin  (None, 17, 17, 672)  0          ['block6a_expand_activation[0][0]
 g2D)                                                            ']

 block6a_dwconv (DepthwiseConv2  (None, 7, 7, 672)   16800       ['block6a_dwconv_pad[0][0]']
 D)

 block6a_bn (BatchNormalization  (None, 7, 7, 672)   2688        ['block6a_dwconv[0][0]']
 )

 block6a_activation (Activation  (None, 7, 7, 672)   0           ['block6a_bn[0][0]']
 )

 block6a_se_squeeze (GlobalAver  (None, 672)         0           ['block6a_activation[0][0]']
 agePooling2D)

 block6a_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block6a_se_squeeze[0][0]']

 block6a_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block6a_se_reshape[0][0]']

 block6a_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block6a_se_reduce[0][0]']

 block6a_se_excite (Multiply)   (None, 7, 7, 672)    0           ['block6a_activation[0][0]',
                                                                  'block6a_se_expand[0][0]']

 block6a_project_conv (Conv2D)  (None, 7, 7, 192)    129024      ['block6a_se_excite[0][0]']

 block6a_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6a_project_conv[0][0]']
 lization)

 block6b_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6a_project_bn[0][0]']

 block6b_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6b_expand_conv[0][0]']
 ization)

 block6b_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6b_expand_bn[0][0]']
 ivation)

 block6b_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6b_expand_activation[0][0]
 D)                                                              ']

 block6b_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6b_dwconv[0][0]']
 )

 block6b_activation (Activation  (None, 7, 7, 1152)  0           ['block6b_bn[0][0]']
 )

 block6b_se_squeeze (GlobalAver  (None, 1152)        0           ['block6b_activation[0][0]']
 agePooling2D)

 block6b_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6b_se_squeeze[0][0]']

 block6b_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6b_se_reshape[0][0]']

 block6b_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6b_se_reduce[0][0]']

 block6b_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6b_activation[0][0]',
                                                                  'block6b_se_expand[0][0]']

 block6b_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6b_se_excite[0][0]']

 block6b_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6b_project_conv[0][0]']
 lization)

 block6b_drop (Dropout)         (None, 7, 7, 192)    0           ['block6b_project_bn[0][0]']

 block6b_add (Add)              (None, 7, 7, 192)    0           ['block6b_drop[0][0]',
                                                                  'block6a_project_bn[0][0]']

 block6c_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6b_add[0][0]']

 block6c_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6c_expand_conv[0][0]']
 ization)

 block6c_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6c_expand_bn[0][0]']
 ivation)

 block6c_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6c_expand_activation[0][0]
 D)                                                              ']

 block6c_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6c_dwconv[0][0]']
 )

 block6c_activation (Activation  (None, 7, 7, 1152)  0           ['block6c_bn[0][0]']
 )

 block6c_se_squeeze (GlobalAver  (None, 1152)        0           ['block6c_activation[0][0]']
 agePooling2D)

 block6c_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6c_se_squeeze[0][0]']

 block6c_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6c_se_reshape[0][0]']

 block6c_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6c_se_reduce[0][0]']

 block6c_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6c_activation[0][0]',
                                                                  'block6c_se_expand[0][0]']

 block6c_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6c_se_excite[0][0]']

 block6c_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6c_project_conv[0][0]']
 lization)

 block6c_drop (Dropout)         (None, 7, 7, 192)    0           ['block6c_project_bn[0][0]']

 block6c_add (Add)              (None, 7, 7, 192)    0           ['block6c_drop[0][0]',
                                                                  'block6b_add[0][0]']

 block6d_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6c_add[0][0]']

 block6d_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6d_expand_conv[0][0]']
 ization)

 block6d_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6d_expand_bn[0][0]']
 ivation)

 block6d_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6d_expand_activation[0][0]
 D)                                                              ']

 block6d_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6d_dwconv[0][0]']
 )

 block6d_activation (Activation  (None, 7, 7, 1152)  0           ['block6d_bn[0][0]']
 )

 block6d_se_squeeze (GlobalAver  (None, 1152)        0           ['block6d_activation[0][0]']
 agePooling2D)

 block6d_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6d_se_squeeze[0][0]']

 block6d_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6d_se_reshape[0][0]']

 block6d_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6d_se_reduce[0][0]']

 block6d_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6d_activation[0][0]',
                                                                  'block6d_se_expand[0][0]']

 block6d_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6d_se_excite[0][0]']

 block6d_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6d_project_conv[0][0]']
 lization)

 block6d_drop (Dropout)         (None, 7, 7, 192)    0           ['block6d_project_bn[0][0]']

 block6d_add (Add)              (None, 7, 7, 192)    0           ['block6d_drop[0][0]',
                                                                  'block6c_add[0][0]']

 block7a_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6d_add[0][0]']

 block7a_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block7a_expand_conv[0][0]']
 ization)

 block7a_expand_activation (Act  (None, 7, 7, 1152)  0           ['block7a_expand_bn[0][0]']
 ivation)

 block7a_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  10368       ['block7a_expand_activation[0][0]
 D)                                                              ']

 block7a_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block7a_dwconv[0][0]']
 )

 block7a_activation (Activation  (None, 7, 7, 1152)  0           ['block7a_bn[0][0]']
 )

 block7a_se_squeeze (GlobalAver  (None, 1152)        0           ['block7a_activation[0][0]']
 agePooling2D)

 block7a_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block7a_se_squeeze[0][0]']

 block7a_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block7a_se_reshape[0][0]']

 block7a_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block7a_se_reduce[0][0]']

 block7a_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block7a_activation[0][0]',
                                                                  'block7a_se_expand[0][0]']

 block7a_project_conv (Conv2D)  (None, 7, 7, 320)    368640      ['block7a_se_excite[0][0]']

 block7a_project_bn (BatchNorma  (None, 7, 7, 320)   1280        ['block7a_project_conv[0][0]']
 lization)

 top_conv (Conv2D)              (None, 7, 7, 1280)   409600      ['block7a_project_bn[0][0]']

 top_bn (BatchNormalization)    (None, 7, 7, 1280)   5120        ['top_conv[0][0]']

 top_activation (Activation)    (None, 7, 7, 1280)   0           ['top_bn[0][0]']

 conv2d (Conv2D)                (None, 7, 7, 128)    1474688     ['top_activation[0][0]']

 max_pooling2d (MaxPooling2D)   (None, 3, 3, 128)    0           ['conv2d[0][0]']

 batch_normalization (BatchNorm  (None, 3, 3, 128)   512         ['max_pooling2d[0][0]']
 alization)

 conv2d_1 (Conv2D)              (None, 3, 3, 128)    147584      ['batch_normalization[0][0]']

 max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 128)   0           ['conv2d_1[0][0]']

 batch_normalization_1 (BatchNo  (None, 1, 1, 128)   512         ['max_pooling2d_1[0][0]']
 rmalization)

 flatten (Flatten)              (None, 128)          0           ['batch_normalization_1[0][0]']

 dense (Dense)                  (None, 512)          66048       ['flatten[0][0]']

 batch_normalization_2 (BatchNo  (None, 512)         2048        ['dense[0][0]']
 rmalization)

 dropout (Dropout)              (None, 512)          0           ['batch_normalization_2[0][0]']

 dense_1 (Dense)                (None, 256)          131328      ['dropout[0][0]']

 batch_normalization_3 (BatchNo  (None, 256)         1024        ['dense_1[0][0]']
 rmalization)

 dropout_1 (Dropout)            (None, 256)          0           ['batch_normalization_3[0][0]']

 dense_2 (Dense)                (None, 128)          32896       ['dropout_1[0][0]']

 batch_normalization_4 (BatchNo  (None, 128)         512         ['dense_2[0][0]']
 rmalization)

 dropout_2 (Dropout)            (None, 128)          0           ['batch_normalization_4[0][0]']

 dense_3 (Dense)                (None, 9)            1161        ['dropout_2[0][0]']

==================================================================================================
Total params: 5,907,884
Trainable params: 5,863,557
Non-trainable params: 44,327
__________________________________________________________________________________________________
Epoch 1/100
2025-03-27 17:08:15.984469: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8907
2025-03-27 17:08:17.296827: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
 95/323 [=======>......................] - ETA: 1:17 - loss: 2.9794 - accuracy: 0.2985C:\Program Files\Python310\lib\site-packages\PIL\Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
323/323 [==============================] - ETA: 0s - loss: 2.1705 - accuracy: 0.5530
Epoch 1: val_loss improved from inf to 4.39533, saving model to efficientnet_saved_2025-03-27_17-08-04.keras
323/323 [==============================] - 138s 398ms/step - loss: 2.1705 - accuracy: 0.5530 - val_loss: 4.3953 - val_accuracy: 0.0897 - lr: 0.0010
Epoch 2/100
323/323 [==============================] - ETA: 0s - loss: 1.3581 - accuracy: 0.8059
Epoch 2: val_loss improved from 4.39533 to 3.01514, saving model to efficientnet_saved_2025-03-27_17-08-04.keras
323/323 [==============================] - 128s 395ms/step - loss: 1.3581 - accuracy: 0.8059 - val_loss: 3.0151 - val_accuracy: 0.2269 - lr: 0.0010
Epoch 3/100
323/323 [==============================] - ETA: 0s - loss: 1.1376 - accuracy: 0.8488
Epoch 3: val_loss improved from 3.01514 to 1.63647, saving model to efficientnet_saved_2025-03-27_17-08-04.keras
323/323 [==============================] - 128s 395ms/step - loss: 1.1376 - accuracy: 0.8488 - val_loss: 1.6365 - val_accuracy: 0.6636 - lr: 0.0010
Epoch 4/100
323/323 [==============================] - ETA: 0s - loss: 0.9782 - accuracy: 0.8730
Epoch 4: val_loss did not improve from 1.63647
323/323 [==============================] - 128s 394ms/step - loss: 0.9782 - accuracy: 0.8730 - val_loss: 15.3101 - val_accuracy: 0.1356 - lr: 0.0010
Epoch 5/100
323/323 [==============================] - ETA: 0s - loss: 0.8722 - accuracy: 0.8840
Epoch 5: val_loss did not improve from 1.63647
323/323 [==============================] - 127s 393ms/step - loss: 0.8722 - accuracy: 0.8840 - val_loss: 5.1643 - val_accuracy: 0.0773 - lr: 0.0010
Epoch 6/100
323/323 [==============================] - ETA: 0s - loss: 0.7594 - accuracy: 0.8925
Epoch 6: val_loss did not improve from 1.63647

Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
323/323 [==============================] - 129s 398ms/step - loss: 0.7594 - accuracy: 0.8925 - val_loss: 4.1158 - val_accuracy: 0.1026 - lr: 0.0010
Epoch 7/100
323/323 [==============================] - ETA: 0s - loss: 0.5853 - accuracy: 0.9293
Epoch 7: val_loss improved from 1.63647 to 1.53732, saving model to efficientnet_saved_2025-03-27_17-08-04.keras
323/323 [==============================] - 130s 400ms/step - loss: 0.5853 - accuracy: 0.9293 - val_loss: 1.5373 - val_accuracy: 0.6422 - lr: 1.0000e-04
Epoch 8/100
323/323 [==============================] - ETA: 0s - loss: 0.5022 - accuracy: 0.9492
Epoch 8: val_loss improved from 1.53732 to 1.21142, saving model to efficientnet_saved_2025-03-27_17-08-04.keras
323/323 [==============================] - 129s 398ms/step - loss: 0.5022 - accuracy: 0.9492 - val_loss: 1.2114 - val_accuracy: 0.7374 - lr: 1.0000e-04
Epoch 9/100
323/323 [==============================] - ETA: 0s - loss: 0.4829 - accuracy: 0.9530
Epoch 9: val_loss did not improve from 1.21142
323/323 [==============================] - 129s 398ms/step - loss: 0.4829 - accuracy: 0.9530 - val_loss: 1.4507 - val_accuracy: 0.6531 - lr: 1.0000e-04
Epoch 10/100
323/323 [==============================] - ETA: 0s - loss: 0.4413 - accuracy: 0.9611
Epoch 10: val_loss did not improve from 1.21142
323/323 [==============================] - 129s 397ms/step - loss: 0.4413 - accuracy: 0.9611 - val_loss: 1.6406 - val_accuracy: 0.6395 - lr: 1.0000e-04
Epoch 11/100
323/323 [==============================] - ETA: 0s - loss: 0.4112 - accuracy: 0.9662
Epoch 11: val_loss did not improve from 1.21142

Epoch 11: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
323/323 [==============================] - 129s 397ms/step - loss: 0.4112 - accuracy: 0.9662 - val_loss: 2.4365 - val_accuracy: 0.5074 - lr: 1.0000e-04
Epoch 12/100
323/323 [==============================] - ETA: 0s - loss: 0.3844 - accuracy: 0.9692
Epoch 12: val_loss improved from 1.21142 to 0.51198, saving model to efficientnet_saved_2025-03-27_17-08-04.keras
323/323 [==============================] - 129s 399ms/step - loss: 0.3844 - accuracy: 0.9692 - val_loss: 0.5120 - val_accuracy: 0.9336 - lr: 1.0000e-05
Epoch 13/100
323/323 [==============================] - ETA: 0s - loss: 0.3874 - accuracy: 0.9701
Epoch 13: val_loss did not improve from 0.51198
323/323 [==============================] - 129s 398ms/step - loss: 0.3874 - accuracy: 0.9701 - val_loss: 0.5603 - val_accuracy: 0.9250 - lr: 1.0000e-05
Epoch 14/100
323/323 [==============================] - ETA: 0s - loss: 0.3773 - accuracy: 0.9718
Epoch 14: val_loss did not improve from 0.51198
323/323 [==============================] - 128s 396ms/step - loss: 0.3773 - accuracy: 0.9718 - val_loss: 0.5365 - val_accuracy: 0.9250 - lr: 1.0000e-05
Epoch 15/100
323/323 [==============================] - ETA: 0s - loss: 0.3708 - accuracy: 0.9744
Epoch 15: val_loss improved from 0.51198 to 0.47216, saving model to efficientnet_saved_2025-03-27_17-08-04.keras
323/323 [==============================] - 129s 399ms/step - loss: 0.3708 - accuracy: 0.9744 - val_loss: 0.4722 - val_accuracy: 0.9503 - lr: 1.0000e-05
Epoch 16/100
323/323 [==============================] - ETA: 0s - loss: 0.3698 - accuracy: 0.9727
Epoch 16: val_loss did not improve from 0.47216
323/323 [==============================] - 129s 397ms/step - loss: 0.3698 - accuracy: 0.9727 - val_loss: 0.5585 - val_accuracy: 0.9262 - lr: 1.0000e-05
Epoch 17/100
323/323 [==============================] - ETA: 0s - loss: 0.3722 - accuracy: 0.9736
Epoch 17: val_loss did not improve from 0.47216
323/323 [==============================] - 128s 396ms/step - loss: 0.3722 - accuracy: 0.9736 - val_loss: 0.4850 - val_accuracy: 0.9413 - lr: 1.0000e-05
Epoch 18/100
323/323 [==============================] - ETA: 0s - loss: 0.3646 - accuracy: 0.9733
Epoch 18: val_loss improved from 0.47216 to 0.45788, saving model to efficientnet_saved_2025-03-27_17-08-04.keras
323/323 [==============================] - 129s 398ms/step - loss: 0.3646 - accuracy: 0.9733 - val_loss: 0.4579 - val_accuracy: 0.9495 - lr: 1.0000e-05
Epoch 19/100
323/323 [==============================] - ETA: 0s - loss: 0.3549 - accuracy: 0.9747
Epoch 19: val_loss did not improve from 0.45788
323/323 [==============================] - 129s 399ms/step - loss: 0.3549 - accuracy: 0.9747 - val_loss: 0.4742 - val_accuracy: 0.9460 - lr: 1.0000e-05
Epoch 20/100
323/323 [==============================] - ETA: 0s - loss: 0.3504 - accuracy: 0.9766
Epoch 20: val_loss did not improve from 0.45788
323/323 [==============================] - 129s 397ms/step - loss: 0.3504 - accuracy: 0.9766 - val_loss: 0.4861 - val_accuracy: 0.9402 - lr: 1.0000e-05
Epoch 21/100
323/323 [==============================] - ETA: 0s - loss: 0.3510 - accuracy: 0.9770
Epoch 21: val_loss did not improve from 0.45788

Epoch 21: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
323/323 [==============================] - 128s 397ms/step - loss: 0.3510 - accuracy: 0.9770 - val_loss: 0.5291 - val_accuracy: 0.9277 - lr: 1.0000e-05
Epoch 22/100
323/323 [==============================] - ETA: 0s - loss: 0.3419 - accuracy: 0.9780
Epoch 22: val_loss did not improve from 0.45788
323/323 [==============================] - 129s 397ms/step - loss: 0.3419 - accuracy: 0.9780 - val_loss: 0.4620 - val_accuracy: 0.9479 - lr: 1.0000e-06
Epoch 23/100
323/323 [==============================] - ETA: 0s - loss: 0.3440 - accuracy: 0.9787
Epoch 23: val_loss improved from 0.45788 to 0.44559, saving model to efficientnet_saved_2025-03-27_17-08-04.keras
323/323 [==============================] - 129s 398ms/step - loss: 0.3440 - accuracy: 0.9787 - val_loss: 0.4456 - val_accuracy: 0.9545 - lr: 1.0000e-06
Epoch 24/100
323/323 [==============================] - ETA: 0s - loss: 0.3410 - accuracy: 0.9763
Epoch 24: val_loss did not improve from 0.44559
323/323 [==============================] - 129s 399ms/step - loss: 0.3410 - accuracy: 0.9763 - val_loss: 0.4530 - val_accuracy: 0.9530 - lr: 1.0000e-06
Epoch 25/100
323/323 [==============================] - ETA: 0s - loss: 0.3403 - accuracy: 0.9764
Epoch 25: val_loss did not improve from 0.44559
323/323 [==============================] - 128s 397ms/step - loss: 0.3403 - accuracy: 0.9764 - val_loss: 0.4653 - val_accuracy: 0.9452 - lr: 1.0000e-06
Epoch 26/100
323/323 [==============================] - ETA: 0s - loss: 0.3453 - accuracy: 0.9756
Epoch 26: val_loss did not improve from 0.44559

Epoch 26: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.
323/323 [==============================] - 129s 397ms/step - loss: 0.3453 - accuracy: 0.9756 - val_loss: 0.4503 - val_accuracy: 0.9530 - lr: 1.0000e-06
Epoch 27/100
323/323 [==============================] - ETA: 0s - loss: 0.3468 - accuracy: 0.9764
Epoch 27: val_loss did not improve from 0.44559
323/323 [==============================] - 129s 398ms/step - loss: 0.3468 - accuracy: 0.9764 - val_loss: 0.4520 - val_accuracy: 0.9472 - lr: 1.0000e-07
Epoch 28/100
323/323 [==============================] - ETA: 0s - loss: 0.3407 - accuracy: 0.9777
Epoch 28: val_loss did not improve from 0.44559
323/323 [==============================] - 129s 398ms/step - loss: 0.3407 - accuracy: 0.9777 - val_loss: 0.4712 - val_accuracy: 0.9441 - lr: 1.0000e-07
Epoch 29/100
323/323 [==============================] - ETA: 0s - loss: 0.3470 - accuracy: 0.9774
Epoch 29: val_loss did not improve from 0.44559

Epoch 29: ReduceLROnPlateau reducing learning rate to 1e-07.
323/323 [==============================] - 129s 398ms/step - loss: 0.3470 - accuracy: 0.9774 - val_loss: 0.4469 - val_accuracy: 0.9549 - lr: 1.0000e-07
Epoch 30/100
323/323 [==============================] - ETA: 0s - loss: 0.3439 - accuracy: 0.9766
Epoch 30: val_loss did not improve from 0.44559
323/323 [==============================] - 129s 397ms/step - loss: 0.3439 - accuracy: 0.9766 - val_loss: 0.4635 - val_accuracy: 0.9468 - lr: 1.0000e-07
Epoch 31/100
323/323 [==============================] - ETA: 0s - loss: 0.3470 - accuracy: 0.9765
Epoch 31: val_loss did not improve from 0.44559
323/323 [==============================] - 129s 399ms/step - loss: 0.3470 - accuracy: 0.9765 - val_loss: 0.4590 - val_accuracy: 0.9495 - lr: 1.0000e-07
Epoch 32/100
323/323 [==============================] - ETA: 0s - loss: 0.3524 - accuracy: 0.9754
Epoch 32: val_loss did not improve from 0.44559
323/323 [==============================] - 128s 397ms/step - loss: 0.3524 - accuracy: 0.9754 - val_loss: 0.4524 - val_accuracy: 0.9538 - lr: 1.0000e-07
Epoch 33/100
323/323 [==============================] - ETA: 0s - loss: 0.3420 - accuracy: 0.9790
Epoch 33: val_loss did not improve from 0.44559
323/323 [==============================] - 129s 397ms/step - loss: 0.3420 - accuracy: 0.9790 - val_loss: 0.4468 - val_accuracy: 0.9514 - lr: 1.0000e-07
Test Accuracy: 0.9688
173/173 [==============================] - 14s 77ms/step
Classification Report:
              precision    recall  f1-score   support

     battery       0.98      0.98      0.98       390
   cardboard       0.98      0.96      0.97       417
     clothes       1.00      0.99      1.00      1509
       glass       0.98      0.91      0.94       602
       metal       0.90      0.97      0.93       408
     organic       0.97      0.98      0.98       545
       paper       0.94      0.96      0.95       464
     plastic       0.92      0.95      0.94       594
       shoes       0.98      0.99      0.99       592

    accuracy                           0.97      5521
   macro avg       0.96      0.97      0.96      5521
weighted avg       0.97      0.97      0.97      5521

Confusion Matrix: [[ 381    0    0    0    7    0    0    1    1]
 [   2  401    0    0    0    1   11    2    0]
 [   0    0 1496    0    3    0    9    0    1]
 [   2    1    0  548   17    5    0   29    0]
 [   2    1    0    0  396    3    1    4    1]
 [   1    0    0    2    2  533    1    2    4]
 [   0    6    0    0    2    2  445    6    3]
 [   0    0    0   11   12    1    6  563    1]
 [   0    1    0    0    1    2    0    2  586]]

Process finished with exit code 0

