27-03-2025-17-06

"C:\Program Files\Python310\python.exe" D:\Study\CNTT\A.MHUD\CNN_Practice\MODEL_EFFICIENTNET.py
GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
TensorFlow is using GPU: True
2025-03-27 17:08:03.418218: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-27 17:08:03.885678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8192 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6
1 Physical GPU, 1 Logical GPUs
Found 10321 images belonging to 9 classes.
Found 2574 images belonging to 9 classes.
Found 5521 images belonging to 9 classes.
Training class distribution: Counter({2: 2820, 3: 1124, 7: 1112, 8: 1108, 5: 1020, 6: 868, 1: 779, 4: 762, 0: 728})
Validation class distribution: Counter({2: 704, 3: 281, 7: 277, 8: 276, 5: 254, 6: 216, 1: 194, 4: 190, 0: 182})
Testing class distribution: Counter({2: 1509, 3: 602, 7: 594, 8: 592, 5: 545, 6: 464, 1: 417, 4: 408, 0: 390})
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to
==================================================================================================
 input_1 (InputLayer)           [(None, 224, 224, 3  0           []
                                )]

 rescaling (Rescaling)          (None, 224, 224, 3)  0           ['input_1[0][0]']

 normalization (Normalization)  (None, 224, 224, 3)  7           ['rescaling[0][0]']

 tf.math.truediv (TFOpLambda)   (None, 224, 224, 3)  0           ['normalization[0][0]']

 stem_conv_pad (ZeroPadding2D)  (None, 225, 225, 3)  0           ['tf.math.truediv[0][0]']

 stem_conv (Conv2D)             (None, 112, 112, 32  864         ['stem_conv_pad[0][0]']
                                )

 stem_bn (BatchNormalization)   (None, 112, 112, 32  128         ['stem_conv[0][0]']
                                )

 stem_activation (Activation)   (None, 112, 112, 32  0           ['stem_bn[0][0]']
                                )

 block1a_dwconv (DepthwiseConv2  (None, 112, 112, 32  288        ['stem_activation[0][0]']
 D)                             )

 block1a_bn (BatchNormalization  (None, 112, 112, 32  128        ['block1a_dwconv[0][0]']
 )                              )

 block1a_activation (Activation  (None, 112, 112, 32  0          ['block1a_bn[0][0]']
 )                              )

 block1a_se_squeeze (GlobalAver  (None, 32)          0           ['block1a_activation[0][0]']
 agePooling2D)

 block1a_se_reshape (Reshape)   (None, 1, 1, 32)     0           ['block1a_se_squeeze[0][0]']

 block1a_se_reduce (Conv2D)     (None, 1, 1, 8)      264         ['block1a_se_reshape[0][0]']

 block1a_se_expand (Conv2D)     (None, 1, 1, 32)     288         ['block1a_se_reduce[0][0]']

 block1a_se_excite (Multiply)   (None, 112, 112, 32  0           ['block1a_activation[0][0]',
                                )                                 'block1a_se_expand[0][0]']

 block1a_project_conv (Conv2D)  (None, 112, 112, 16  512         ['block1a_se_excite[0][0]']
                                )

 block1a_project_bn (BatchNorma  (None, 112, 112, 16  64         ['block1a_project_conv[0][0]']
 lization)                      )

 block2a_expand_conv (Conv2D)   (None, 112, 112, 96  1536        ['block1a_project_bn[0][0]']
                                )

 block2a_expand_bn (BatchNormal  (None, 112, 112, 96  384        ['block2a_expand_conv[0][0]']
 ization)                       )

 block2a_expand_activation (Act  (None, 112, 112, 96  0          ['block2a_expand_bn[0][0]']
 ivation)                       )

 block2a_dwconv_pad (ZeroPaddin  (None, 113, 113, 96  0          ['block2a_expand_activation[0][0]
 g2D)                           )                                ']

 block2a_dwconv (DepthwiseConv2  (None, 56, 56, 96)  864         ['block2a_dwconv_pad[0][0]']
 D)

 block2a_bn (BatchNormalization  (None, 56, 56, 96)  384         ['block2a_dwconv[0][0]']
 )

 block2a_activation (Activation  (None, 56, 56, 96)  0           ['block2a_bn[0][0]']
 )

 block2a_se_squeeze (GlobalAver  (None, 96)          0           ['block2a_activation[0][0]']
 agePooling2D)

 block2a_se_reshape (Reshape)   (None, 1, 1, 96)     0           ['block2a_se_squeeze[0][0]']

 block2a_se_reduce (Conv2D)     (None, 1, 1, 4)      388         ['block2a_se_reshape[0][0]']

 block2a_se_expand (Conv2D)     (None, 1, 1, 96)     480         ['block2a_se_reduce[0][0]']

 block2a_se_excite (Multiply)   (None, 56, 56, 96)   0           ['block2a_activation[0][0]',
                                                                  'block2a_se_expand[0][0]']

 block2a_project_conv (Conv2D)  (None, 56, 56, 24)   2304        ['block2a_se_excite[0][0]']

 block2a_project_bn (BatchNorma  (None, 56, 56, 24)  96          ['block2a_project_conv[0][0]']
 lization)

 block2b_expand_conv (Conv2D)   (None, 56, 56, 144)  3456        ['block2a_project_bn[0][0]']

 block2b_expand_bn (BatchNormal  (None, 56, 56, 144)  576        ['block2b_expand_conv[0][0]']
 ization)

 block2b_expand_activation (Act  (None, 56, 56, 144)  0          ['block2b_expand_bn[0][0]']
 ivation)

 block2b_dwconv (DepthwiseConv2  (None, 56, 56, 144)  1296       ['block2b_expand_activation[0][0]
 D)                                                              ']

 block2b_bn (BatchNormalization  (None, 56, 56, 144)  576        ['block2b_dwconv[0][0]']
 )

 block2b_activation (Activation  (None, 56, 56, 144)  0          ['block2b_bn[0][0]']
 )

 block2b_se_squeeze (GlobalAver  (None, 144)         0           ['block2b_activation[0][0]']
 agePooling2D)

 block2b_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block2b_se_squeeze[0][0]']

 block2b_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block2b_se_reshape[0][0]']

 block2b_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block2b_se_reduce[0][0]']

 block2b_se_excite (Multiply)   (None, 56, 56, 144)  0           ['block2b_activation[0][0]',
                                                                  'block2b_se_expand[0][0]']

 block2b_project_conv (Conv2D)  (None, 56, 56, 24)   3456        ['block2b_se_excite[0][0]']

 block2b_project_bn (BatchNorma  (None, 56, 56, 24)  96          ['block2b_project_conv[0][0]']
 lization)

 block2b_drop (Dropout)         (None, 56, 56, 24)   0           ['block2b_project_bn[0][0]']

 block2b_add (Add)              (None, 56, 56, 24)   0           ['block2b_drop[0][0]',
                                                                  'block2a_project_bn[0][0]']

 block3a_expand_conv (Conv2D)   (None, 56, 56, 144)  3456        ['block2b_add[0][0]']

 block3a_expand_bn (BatchNormal  (None, 56, 56, 144)  576        ['block3a_expand_conv[0][0]']
 ization)

 block3a_expand_activation (Act  (None, 56, 56, 144)  0          ['block3a_expand_bn[0][0]']
 ivation)

 block3a_dwconv_pad (ZeroPaddin  (None, 59, 59, 144)  0          ['block3a_expand_activation[0][0]
 g2D)                                                            ']

 block3a_dwconv (DepthwiseConv2  (None, 28, 28, 144)  3600       ['block3a_dwconv_pad[0][0]']
 D)

 block3a_bn (BatchNormalization  (None, 28, 28, 144)  576        ['block3a_dwconv[0][0]']
 )

 block3a_activation (Activation  (None, 28, 28, 144)  0          ['block3a_bn[0][0]']
 )

 block3a_se_squeeze (GlobalAver  (None, 144)         0           ['block3a_activation[0][0]']
 agePooling2D)

 block3a_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block3a_se_squeeze[0][0]']

 block3a_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block3a_se_reshape[0][0]']

 block3a_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block3a_se_reduce[0][0]']

 block3a_se_excite (Multiply)   (None, 28, 28, 144)  0           ['block3a_activation[0][0]',
                                                                  'block3a_se_expand[0][0]']

 block3a_project_conv (Conv2D)  (None, 28, 28, 40)   5760        ['block3a_se_excite[0][0]']

 block3a_project_bn (BatchNorma  (None, 28, 28, 40)  160         ['block3a_project_conv[0][0]']
 lization)

 block3b_expand_conv (Conv2D)   (None, 28, 28, 240)  9600        ['block3a_project_bn[0][0]']

 block3b_expand_bn (BatchNormal  (None, 28, 28, 240)  960        ['block3b_expand_conv[0][0]']
 ization)

 block3b_expand_activation (Act  (None, 28, 28, 240)  0          ['block3b_expand_bn[0][0]']
 ivation)

 block3b_dwconv (DepthwiseConv2  (None, 28, 28, 240)  6000       ['block3b_expand_activation[0][0]
 D)                                                              ']

 block3b_bn (BatchNormalization  (None, 28, 28, 240)  960        ['block3b_dwconv[0][0]']
 )

 block3b_activation (Activation  (None, 28, 28, 240)  0          ['block3b_bn[0][0]']
 )

 block3b_se_squeeze (GlobalAver  (None, 240)         0           ['block3b_activation[0][0]']
 agePooling2D)

 block3b_se_reshape (Reshape)   (None, 1, 1, 240)    0           ['block3b_se_squeeze[0][0]']

 block3b_se_reduce (Conv2D)     (None, 1, 1, 10)     2410        ['block3b_se_reshape[0][0]']

 block3b_se_expand (Conv2D)     (None, 1, 1, 240)    2640        ['block3b_se_reduce[0][0]']

 block3b_se_excite (Multiply)   (None, 28, 28, 240)  0           ['block3b_activation[0][0]',
                                                                  'block3b_se_expand[0][0]']

 block3b_project_conv (Conv2D)  (None, 28, 28, 40)   9600        ['block3b_se_excite[0][0]']

 block3b_project_bn (BatchNorma  (None, 28, 28, 40)  160         ['block3b_project_conv[0][0]']
 lization)

 block3b_drop (Dropout)         (None, 28, 28, 40)   0           ['block3b_project_bn[0][0]']

 block3b_add (Add)              (None, 28, 28, 40)   0           ['block3b_drop[0][0]',
                                                                  'block3a_project_bn[0][0]']

 block4a_expand_conv (Conv2D)   (None, 28, 28, 240)  9600        ['block3b_add[0][0]']

 block4a_expand_bn (BatchNormal  (None, 28, 28, 240)  960        ['block4a_expand_conv[0][0]']
 ization)

 block4a_expand_activation (Act  (None, 28, 28, 240)  0          ['block4a_expand_bn[0][0]']
 ivation)

 block4a_dwconv_pad (ZeroPaddin  (None, 29, 29, 240)  0          ['block4a_expand_activation[0][0]
 g2D)                                                            ']

 block4a_dwconv (DepthwiseConv2  (None, 14, 14, 240)  2160       ['block4a_dwconv_pad[0][0]']
 D)

 block4a_bn (BatchNormalization  (None, 14, 14, 240)  960        ['block4a_dwconv[0][0]']
 )

 block4a_activation (Activation  (None, 14, 14, 240)  0          ['block4a_bn[0][0]']
 )

 block4a_se_squeeze (GlobalAver  (None, 240)         0           ['block4a_activation[0][0]']
 agePooling2D)

 block4a_se_reshape (Reshape)   (None, 1, 1, 240)    0           ['block4a_se_squeeze[0][0]']

 block4a_se_reduce (Conv2D)     (None, 1, 1, 10)     2410        ['block4a_se_reshape[0][0]']

 block4a_se_expand (Conv2D)     (None, 1, 1, 240)    2640        ['block4a_se_reduce[0][0]']

 block4a_se_excite (Multiply)   (None, 14, 14, 240)  0           ['block4a_activation[0][0]',
                                                                  'block4a_se_expand[0][0]']

 block4a_project_conv (Conv2D)  (None, 14, 14, 80)   19200       ['block4a_se_excite[0][0]']

 block4a_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4a_project_conv[0][0]']
 lization)

 block4b_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4a_project_bn[0][0]']

 block4b_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block4b_expand_conv[0][0]']
 ization)

 block4b_expand_activation (Act  (None, 14, 14, 480)  0          ['block4b_expand_bn[0][0]']
 ivation)

 block4b_dwconv (DepthwiseConv2  (None, 14, 14, 480)  4320       ['block4b_expand_activation[0][0]
 D)                                                              ']

 block4b_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block4b_dwconv[0][0]']
 )

 block4b_activation (Activation  (None, 14, 14, 480)  0          ['block4b_bn[0][0]']
 )

 block4b_se_squeeze (GlobalAver  (None, 480)         0           ['block4b_activation[0][0]']
 agePooling2D)

 block4b_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block4b_se_squeeze[0][0]']

 block4b_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block4b_se_reshape[0][0]']

 block4b_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block4b_se_reduce[0][0]']

 block4b_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block4b_activation[0][0]',
                                                                  'block4b_se_expand[0][0]']

 block4b_project_conv (Conv2D)  (None, 14, 14, 80)   38400       ['block4b_se_excite[0][0]']

 block4b_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4b_project_conv[0][0]']
 lization)

 block4b_drop (Dropout)         (None, 14, 14, 80)   0           ['block4b_project_bn[0][0]']

 block4b_add (Add)              (None, 14, 14, 80)   0           ['block4b_drop[0][0]',
                                                                  'block4a_project_bn[0][0]']

 block4c_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4b_add[0][0]']

 block4c_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block4c_expand_conv[0][0]']
 ization)

 block4c_expand_activation (Act  (None, 14, 14, 480)  0          ['block4c_expand_bn[0][0]']
 ivation)

 block4c_dwconv (DepthwiseConv2  (None, 14, 14, 480)  4320       ['block4c_expand_activation[0][0]
 D)                                                              ']

 block4c_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block4c_dwconv[0][0]']
 )

 block4c_activation (Activation  (None, 14, 14, 480)  0          ['block4c_bn[0][0]']
 )

 block4c_se_squeeze (GlobalAver  (None, 480)         0           ['block4c_activation[0][0]']
 agePooling2D)

 block4c_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block4c_se_squeeze[0][0]']

 block4c_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block4c_se_reshape[0][0]']

 block4c_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block4c_se_reduce[0][0]']

 block4c_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block4c_activation[0][0]',
                                                                  'block4c_se_expand[0][0]']

 block4c_project_conv (Conv2D)  (None, 14, 14, 80)   38400       ['block4c_se_excite[0][0]']

 block4c_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4c_project_conv[0][0]']
 lization)

 block4c_drop (Dropout)         (None, 14, 14, 80)   0           ['block4c_project_bn[0][0]']

 block4c_add (Add)              (None, 14, 14, 80)   0           ['block4c_drop[0][0]',
                                                                  'block4b_add[0][0]']

 block5a_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4c_add[0][0]']

 block5a_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block5a_expand_conv[0][0]']
 ization)

 block5a_expand_activation (Act  (None, 14, 14, 480)  0          ['block5a_expand_bn[0][0]']
 ivation)

 block5a_dwconv (DepthwiseConv2  (None, 14, 14, 480)  12000      ['block5a_expand_activation[0][0]
 D)                                                              ']

 block5a_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block5a_dwconv[0][0]']
 )

 block5a_activation (Activation  (None, 14, 14, 480)  0          ['block5a_bn[0][0]']
 )

 block5a_se_squeeze (GlobalAver  (None, 480)         0           ['block5a_activation[0][0]']
 agePooling2D)

 block5a_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block5a_se_squeeze[0][0]']

 block5a_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block5a_se_reshape[0][0]']

 block5a_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block5a_se_reduce[0][0]']

 block5a_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block5a_activation[0][0]',
                                                                  'block5a_se_expand[0][0]']

 block5a_project_conv (Conv2D)  (None, 14, 14, 112)  53760       ['block5a_se_excite[0][0]']

 block5a_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5a_project_conv[0][0]']
 lization)

 block5b_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5a_project_bn[0][0]']

 block5b_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block5b_expand_conv[0][0]']
 ization)

 block5b_expand_activation (Act  (None, 14, 14, 672)  0          ['block5b_expand_bn[0][0]']
 ivation)

 block5b_dwconv (DepthwiseConv2  (None, 14, 14, 672)  16800      ['block5b_expand_activation[0][0]
 D)                                                              ']

 block5b_bn (BatchNormalization  (None, 14, 14, 672)  2688       ['block5b_dwconv[0][0]']
 )

 block5b_activation (Activation  (None, 14, 14, 672)  0          ['block5b_bn[0][0]']
 )

 block5b_se_squeeze (GlobalAver  (None, 672)         0           ['block5b_activation[0][0]']
 agePooling2D)

 block5b_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5b_se_squeeze[0][0]']

 block5b_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5b_se_reshape[0][0]']

 block5b_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5b_se_reduce[0][0]']

 block5b_se_excite (Multiply)   (None, 14, 14, 672)  0           ['block5b_activation[0][0]',
                                                                  'block5b_se_expand[0][0]']

 block5b_project_conv (Conv2D)  (None, 14, 14, 112)  75264       ['block5b_se_excite[0][0]']

 block5b_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5b_project_conv[0][0]']
 lization)

 block5b_drop (Dropout)         (None, 14, 14, 112)  0           ['block5b_project_bn[0][0]']

 block5b_add (Add)              (None, 14, 14, 112)  0           ['block5b_drop[0][0]',
                                                                  'block5a_project_bn[0][0]']

 block5c_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5b_add[0][0]']

 block5c_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block5c_expand_conv[0][0]']
 ization)

 block5c_expand_activation (Act  (None, 14, 14, 672)  0          ['block5c_expand_bn[0][0]']
 ivation)

 block5c_dwconv (DepthwiseConv2  (None, 14, 14, 672)  16800      ['block5c_expand_activation[0][0]
 D)                                                              ']

 block5c_bn (BatchNormalization  (None, 14, 14, 672)  2688       ['block5c_dwconv[0][0]']
 )

 block5c_activation (Activation  (None, 14, 14, 672)  0          ['block5c_bn[0][0]']
 )

 block5c_se_squeeze (GlobalAver  (None, 672)         0           ['block5c_activation[0][0]']
 agePooling2D)

 block5c_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5c_se_squeeze[0][0]']

 block5c_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5c_se_reshape[0][0]']

 block5c_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5c_se_reduce[0][0]']

 block5c_se_excite (Multiply)   (None, 14, 14, 672)  0           ['block5c_activation[0][0]',
                                                                  'block5c_se_expand[0][0]']

 block5c_project_conv (Conv2D)  (None, 14, 14, 112)  75264       ['block5c_se_excite[0][0]']

 block5c_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5c_project_conv[0][0]']
 lization)

 block5c_drop (Dropout)         (None, 14, 14, 112)  0           ['block5c_project_bn[0][0]']

 block5c_add (Add)              (None, 14, 14, 112)  0           ['block5c_drop[0][0]',
                                                                  'block5b_add[0][0]']

 block6a_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5c_add[0][0]']

 block6a_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block6a_expand_conv[0][0]']
 ization)

 block6a_expand_activation (Act  (None, 14, 14, 672)  0          ['block6a_expand_bn[0][0]']
 ivation)

 block6a_dwconv_pad (ZeroPaddin  (None, 17, 17, 672)  0          ['block6a_expand_activation[0][0]
 g2D)                                                            ']

 block6a_dwconv (DepthwiseConv2  (None, 7, 7, 672)   16800       ['block6a_dwconv_pad[0][0]']
 D)

 block6a_bn (BatchNormalization  (None, 7, 7, 672)   2688        ['block6a_dwconv[0][0]']
 )

 block6a_activation (Activation  (None, 7, 7, 672)   0           ['block6a_bn[0][0]']
 )

 block6a_se_squeeze (GlobalAver  (None, 672)         0           ['block6a_activation[0][0]']
 agePooling2D)

 block6a_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block6a_se_squeeze[0][0]']

 block6a_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block6a_se_reshape[0][0]']

 block6a_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block6a_se_reduce[0][0]']

 block6a_se_excite (Multiply)   (None, 7, 7, 672)    0           ['block6a_activation[0][0]',
                                                                  'block6a_se_expand[0][0]']

 block6a_project_conv (Conv2D)  (None, 7, 7, 192)    129024      ['block6a_se_excite[0][0]']

 block6a_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6a_project_conv[0][0]']
 lization)

 block6b_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6a_project_bn[0][0]']

 block6b_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6b_expand_conv[0][0]']
 ization)

 block6b_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6b_expand_bn[0][0]']
 ivation)

 block6b_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6b_expand_activation[0][0]
 D)                                                              ']

 block6b_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6b_dwconv[0][0]']
 )

 block6b_activation (Activation  (None, 7, 7, 1152)  0           ['block6b_bn[0][0]']
 )

 block6b_se_squeeze (GlobalAver  (None, 1152)        0           ['block6b_activation[0][0]']
 agePooling2D)

 block6b_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6b_se_squeeze[0][0]']

 block6b_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6b_se_reshape[0][0]']

 block6b_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6b_se_reduce[0][0]']

 block6b_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6b_activation[0][0]',
                                                                  'block6b_se_expand[0][0]']

 block6b_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6b_se_excite[0][0]']

 block6b_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6b_project_conv[0][0]']
 lization)

 block6b_drop (Dropout)         (None, 7, 7, 192)    0           ['block6b_project_bn[0][0]']

 block6b_add (Add)              (None, 7, 7, 192)    0           ['block6b_drop[0][0]',
                                                                  'block6a_project_bn[0][0]']

 block6c_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6b_add[0][0]']

 block6c_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6c_expand_conv[0][0]']
 ization)

 block6c_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6c_expand_bn[0][0]']
 ivation)

 block6c_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6c_expand_activation[0][0]
 D)                                                              ']

 block6c_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6c_dwconv[0][0]']
 )

 block6c_activation (Activation  (None, 7, 7, 1152)  0           ['block6c_bn[0][0]']
 )

 block6c_se_squeeze (GlobalAver  (None, 1152)        0           ['block6c_activation[0][0]']
 agePooling2D)

 block6c_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6c_se_squeeze[0][0]']

 block6c_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6c_se_reshape[0][0]']

 block6c_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6c_se_reduce[0][0]']

 block6c_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6c_activation[0][0]',
                                                                  'block6c_se_expand[0][0]']

 block6c_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6c_se_excite[0][0]']

 block6c_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6c_project_conv[0][0]']
 lization)

 block6c_drop (Dropout)         (None, 7, 7, 192)    0           ['block6c_project_bn[0][0]']

 block6c_add (Add)              (None, 7, 7, 192)    0           ['block6c_drop[0][0]',
                                                                  'block6b_add[0][0]']

 block6d_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6c_add[0][0]']

 block6d_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6d_expand_conv[0][0]']
 ization)

 block6d_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6d_expand_bn[0][0]']
 ivation)

 block6d_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6d_expand_activation[0][0]
 D)                                                              ']

 block6d_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6d_dwconv[0][0]']
 )

 block6d_activation (Activation  (None, 7, 7, 1152)  0           ['block6d_bn[0][0]']
 )

 block6d_se_squeeze (GlobalAver  (None, 1152)        0           ['block6d_activation[0][0]']
 agePooling2D)

 block6d_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6d_se_squeeze[0][0]']

 block6d_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6d_se_reshape[0][0]']

 block6d_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6d_se_reduce[0][0]']

 block6d_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6d_activation[0][0]',
                                                                  'block6d_se_expand[0][0]']

 block6d_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6d_se_excite[0][0]']

 block6d_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6d_project_conv[0][0]']
 lization)

 block6d_drop (Dropout)         (None, 7, 7, 192)    0           ['block6d_project_bn[0][0]']

 block6d_add (Add)              (None, 7, 7, 192)    0           ['block6d_drop[0][0]',
                                                                  'block6c_add[0][0]']

 block7a_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6d_add[0][0]']

 block7a_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block7a_expand_conv[0][0]']
 ization)

 block7a_expand_activation (Act  (None, 7, 7, 1152)  0           ['block7a_expand_bn[0][0]']
 ivation)

 block7a_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  10368       ['block7a_expand_activation[0][0]
 D)                                                              ']

 block7a_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block7a_dwconv[0][0]']
 )

 block7a_activation (Activation  (None, 7, 7, 1152)  0           ['block7a_bn[0][0]']
 )

 block7a_se_squeeze (GlobalAver  (None, 1152)        0           ['block7a_activation[0][0]']
 agePooling2D)

 block7a_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block7a_se_squeeze[0][0]']

 block7a_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block7a_se_reshape[0][0]']

 block7a_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block7a_se_reduce[0][0]']

 block7a_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block7a_activation[0][0]',
                                                                  'block7a_se_expand[0][0]']

 block7a_project_conv (Conv2D)  (None, 7, 7, 320)    368640      ['block7a_se_excite[0][0]']

 block7a_project_bn (BatchNorma  (None, 7, 7, 320)   1280        ['block7a_project_conv[0][0]']
 lization)

 top_conv (Conv2D)              (None, 7, 7, 1280)   409600      ['block7a_project_bn[0][0]']

 top_bn (BatchNormalization)    (None, 7, 7, 1280)   5120        ['top_conv[0][0]']

 top_activation (Activation)    (None, 7, 7, 1280)   0           ['top_bn[0][0]']

 conv2d (Conv2D)                (None, 7, 7, 128)    1474688     ['top_activation[0][0]']

 max_pooling2d (MaxPooling2D)   (None, 3, 3, 128)    0           ['conv2d[0][0]']

 batch_normalization (BatchNorm  (None, 3, 3, 128)   512         ['max_pooling2d[0][0]']
 alization)

 conv2d_1 (Conv2D)              (None, 3, 3, 128)    147584      ['batch_normalization[0][0]']

 max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 128)   0           ['conv2d_1[0][0]']

 batch_normalization_1 (BatchNo  (None, 1, 1, 128)   512         ['max_pooling2d_1[0][0]']
 rmalization)

 flatten (Flatten)              (None, 128)          0           ['batch_normalization_1[0][0]']

 dense (Dense)                  (None, 512)          66048       ['flatten[0][0]']

 batch_normalization_2 (BatchNo  (None, 512)         2048        ['dense[0][0]']
 rmalization)

 dropout (Dropout)              (None, 512)          0           ['batch_normalization_2[0][0]']

 dense_1 (Dense)                (None, 256)          131328      ['dropout[0][0]']

 batch_normalization_3 (BatchNo  (None, 256)         1024        ['dense_1[0][0]']
 rmalization)

 dropout_1 (Dropout)            (None, 256)          0           ['batch_normalization_3[0][0]']

 dense_2 (Dense)                (None, 128)          32896       ['dropout_1[0][0]']

 batch_normalization_4 (BatchNo  (None, 128)         512         ['dense_2[0][0]']
 rmalization)

 dropout_2 (Dropout)            (None, 128)          0           ['batch_normalization_4[0][0]']

 dense_3 (Dense)                (None, 9)            1161        ['dropout_2[0][0]']

==================================================================================================
Total params: 5,907,884
Trainable params: 5,863,557
Non-trainable params: 44,327
__________________________________________________________________________________________________
Epoch 1/100
2025-03-27 17:08:15.984469: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8907
2025-03-27 17:08:17.296827: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
 95/323 [=======>......................] - ETA: 1:17 - loss: 2.9794 - accuracy: 0.2985C:\Program Files\Python310\lib\site-packages\PIL\Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
323/323 [==============================] - ETA: 0s - loss: 2.1705 - accuracy: 0.5530
Epoch 1: val_loss improved from inf to 4.39533, saving model to efficientnet_saved_2025-03-27_17-08-04.keras
323/323 [==============================] - 138s 398ms/step - loss: 2.1705 - accuracy: 0.5530 - val_loss: 4.3953 - val_accuracy: 0.0897 - lr: 0.0010
Epoch 2/100
323/323 [==============================] - ETA: 0s - loss: 1.3581 - accuracy: 0.8059
Epoch 2: val_loss improved from 4.39533 to 3.01514, saving model to efficientnet_saved_2025-03-27_17-08-04.keras
323/323 [==============================] - 128s 395ms/step - loss: 1.3581 - accuracy: 0.8059 - val_loss: 3.0151 - val_accuracy: 0.2269 - lr: 0.0010
Epoch 3/100
323/323 [==============================] - ETA: 0s - loss: 1.1376 - accuracy: 0.8488
Epoch 3: val_loss improved from 3.01514 to 1.63647, saving model to efficientnet_saved_2025-03-27_17-08-04.keras
323/323 [==============================] - 128s 395ms/step - loss: 1.1376 - accuracy: 0.8488 - val_loss: 1.6365 - val_accuracy: 0.6636 - lr: 0.0010
Epoch 4/100
323/323 [==============================] - ETA: 0s - loss: 0.9782 - accuracy: 0.8730
Epoch 4: val_loss did not improve from 1.63647
323/323 [==============================] - 128s 394ms/step - loss: 0.9782 - accuracy: 0.8730 - val_loss: 15.3101 - val_accuracy: 0.1356 - lr: 0.0010
Epoch 5/100
323/323 [==============================] - ETA: 0s - loss: 0.8722 - accuracy: 0.8840
Epoch 5: val_loss did not improve from 1.63647
323/323 [==============================] - 127s 393ms/step - loss: 0.8722 - accuracy: 0.8840 - val_loss: 5.1643 - val_accuracy: 0.0773 - lr: 0.0010
Epoch 6/100
323/323 [==============================] - ETA: 0s - loss: 0.7594 - accuracy: 0.8925
Epoch 6: val_loss did not improve from 1.63647

Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
323/323 [==============================] - 129s 398ms/step - loss: 0.7594 - accuracy: 0.8925 - val_loss: 4.1158 - val_accuracy: 0.1026 - lr: 0.0010
Epoch 7/100
323/323 [==============================] - ETA: 0s - loss: 0.5853 - accuracy: 0.9293
Epoch 7: val_loss improved from 1.63647 to 1.53732, saving model to efficientnet_saved_2025-03-27_17-08-04.keras
323/323 [==============================] - 130s 400ms/step - loss: 0.5853 - accuracy: 0.9293 - val_loss: 1.5373 - val_accuracy: 0.6422 - lr: 1.0000e-04
Epoch 8/100
323/323 [==============================] - ETA: 0s - loss: 0.5022 - accuracy: 0.9492
Epoch 8: val_loss improved from 1.53732 to 1.21142, saving model to efficientnet_saved_2025-03-27_17-08-04.keras
323/323 [==============================] - 129s 398ms/step - loss: 0.5022 - accuracy: 0.9492 - val_loss: 1.2114 - val_accuracy: 0.7374 - lr: 1.0000e-04
Epoch 9/100
323/323 [==============================] - ETA: 0s - loss: 0.4829 - accuracy: 0.9530
Epoch 9: val_loss did not improve from 1.21142
323/323 [==============================] - 129s 398ms/step - loss: 0.4829 - accuracy: 0.9530 - val_loss: 1.4507 - val_accuracy: 0.6531 - lr: 1.0000e-04
Epoch 10/100
323/323 [==============================] - ETA: 0s - loss: 0.4413 - accuracy: 0.9611
Epoch 10: val_loss did not improve from 1.21142
323/323 [==============================] - 129s 397ms/step - loss: 0.4413 - accuracy: 0.9611 - val_loss: 1.6406 - val_accuracy: 0.6395 - lr: 1.0000e-04
Epoch 11/100
323/323 [==============================] - ETA: 0s - loss: 0.4112 - accuracy: 0.9662
Epoch 11: val_loss did not improve from 1.21142

Epoch 11: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
323/323 [==============================] - 129s 397ms/step - loss: 0.4112 - accuracy: 0.9662 - val_loss: 2.4365 - val_accuracy: 0.5074 - lr: 1.0000e-04
Epoch 12/100
323/323 [==============================] - ETA: 0s - loss: 0.3844 - accuracy: 0.9692
Epoch 12: val_loss improved from 1.21142 to 0.51198, saving model to efficientnet_saved_2025-03-27_17-08-04.keras
323/323 [==============================] - 129s 399ms/step - loss: 0.3844 - accuracy: 0.9692 - val_loss: 0.5120 - val_accuracy: 0.9336 - lr: 1.0000e-05
Epoch 13/100
323/323 [==============================] - ETA: 0s - loss: 0.3874 - accuracy: 0.9701
Epoch 13: val_loss did not improve from 0.51198
323/323 [==============================] - 129s 398ms/step - loss: 0.3874 - accuracy: 0.9701 - val_loss: 0.5603 - val_accuracy: 0.9250 - lr: 1.0000e-05
Epoch 14/100
323/323 [==============================] - ETA: 0s - loss: 0.3773 - accuracy: 0.9718
Epoch 14: val_loss did not improve from 0.51198
323/323 [==============================] - 128s 396ms/step - loss: 0.3773 - accuracy: 0.9718 - val_loss: 0.5365 - val_accuracy: 0.9250 - lr: 1.0000e-05
Epoch 15/100
323/323 [==============================] - ETA: 0s - loss: 0.3708 - accuracy: 0.9744
Epoch 15: val_loss improved from 0.51198 to 0.47216, saving model to efficientnet_saved_2025-03-27_17-08-04.keras
323/323 [==============================] - 129s 399ms/step - loss: 0.3708 - accuracy: 0.9744 - val_loss: 0.4722 - val_accuracy: 0.9503 - lr: 1.0000e-05
Epoch 16/100
323/323 [==============================] - ETA: 0s - loss: 0.3698 - accuracy: 0.9727
Epoch 16: val_loss did not improve from 0.47216
323/323 [==============================] - 129s 397ms/step - loss: 0.3698 - accuracy: 0.9727 - val_loss: 0.5585 - val_accuracy: 0.9262 - lr: 1.0000e-05
Epoch 17/100
323/323 [==============================] - ETA: 0s - loss: 0.3722 - accuracy: 0.9736
Epoch 17: val_loss did not improve from 0.47216
323/323 [==============================] - 128s 396ms/step - loss: 0.3722 - accuracy: 0.9736 - val_loss: 0.4850 - val_accuracy: 0.9413 - lr: 1.0000e-05
Epoch 18/100
323/323 [==============================] - ETA: 0s - loss: 0.3646 - accuracy: 0.9733
Epoch 18: val_loss improved from 0.47216 to 0.45788, saving model to efficientnet_saved_2025-03-27_17-08-04.keras
323/323 [==============================] - 129s 398ms/step - loss: 0.3646 - accuracy: 0.9733 - val_loss: 0.4579 - val_accuracy: 0.9495 - lr: 1.0000e-05
Epoch 19/100
323/323 [==============================] - ETA: 0s - loss: 0.3549 - accuracy: 0.9747
Epoch 19: val_loss did not improve from 0.45788
323/323 [==============================] - 129s 399ms/step - loss: 0.3549 - accuracy: 0.9747 - val_loss: 0.4742 - val_accuracy: 0.9460 - lr: 1.0000e-05
Epoch 20/100
323/323 [==============================] - ETA: 0s - loss: 0.3504 - accuracy: 0.9766
Epoch 20: val_loss did not improve from 0.45788
323/323 [==============================] - 129s 397ms/step - loss: 0.3504 - accuracy: 0.9766 - val_loss: 0.4861 - val_accuracy: 0.9402 - lr: 1.0000e-05
Epoch 21/100
323/323 [==============================] - ETA: 0s - loss: 0.3510 - accuracy: 0.9770
Epoch 21: val_loss did not improve from 0.45788

Epoch 21: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
323/323 [==============================] - 128s 397ms/step - loss: 0.3510 - accuracy: 0.9770 - val_loss: 0.5291 - val_accuracy: 0.9277 - lr: 1.0000e-05
Epoch 22/100
323/323 [==============================] - ETA: 0s - loss: 0.3419 - accuracy: 0.9780
Epoch 22: val_loss did not improve from 0.45788
323/323 [==============================] - 129s 397ms/step - loss: 0.3419 - accuracy: 0.9780 - val_loss: 0.4620 - val_accuracy: 0.9479 - lr: 1.0000e-06
Epoch 23/100
323/323 [==============================] - ETA: 0s - loss: 0.3440 - accuracy: 0.9787
Epoch 23: val_loss improved from 0.45788 to 0.44559, saving model to efficientnet_saved_2025-03-27_17-08-04.keras
323/323 [==============================] - 129s 398ms/step - loss: 0.3440 - accuracy: 0.9787 - val_loss: 0.4456 - val_accuracy: 0.9545 - lr: 1.0000e-06
Epoch 24/100
323/323 [==============================] - ETA: 0s - loss: 0.3410 - accuracy: 0.9763
Epoch 24: val_loss did not improve from 0.44559
323/323 [==============================] - 129s 399ms/step - loss: 0.3410 - accuracy: 0.9763 - val_loss: 0.4530 - val_accuracy: 0.9530 - lr: 1.0000e-06
Epoch 25/100
323/323 [==============================] - ETA: 0s - loss: 0.3403 - accuracy: 0.9764
Epoch 25: val_loss did not improve from 0.44559
323/323 [==============================] - 128s 397ms/step - loss: 0.3403 - accuracy: 0.9764 - val_loss: 0.4653 - val_accuracy: 0.9452 - lr: 1.0000e-06
Epoch 26/100
323/323 [==============================] - ETA: 0s - loss: 0.3453 - accuracy: 0.9756
Epoch 26: val_loss did not improve from 0.44559

Epoch 26: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.
323/323 [==============================] - 129s 397ms/step - loss: 0.3453 - accuracy: 0.9756 - val_loss: 0.4503 - val_accuracy: 0.9530 - lr: 1.0000e-06
Epoch 27/100
323/323 [==============================] - ETA: 0s - loss: 0.3468 - accuracy: 0.9764
Epoch 27: val_loss did not improve from 0.44559
323/323 [==============================] - 129s 398ms/step - loss: 0.3468 - accuracy: 0.9764 - val_loss: 0.4520 - val_accuracy: 0.9472 - lr: 1.0000e-07
Epoch 28/100
323/323 [==============================] - ETA: 0s - loss: 0.3407 - accuracy: 0.9777
Epoch 28: val_loss did not improve from 0.44559
323/323 [==============================] - 129s 398ms/step - loss: 0.3407 - accuracy: 0.9777 - val_loss: 0.4712 - val_accuracy: 0.9441 - lr: 1.0000e-07
Epoch 29/100
323/323 [==============================] - ETA: 0s - loss: 0.3470 - accuracy: 0.9774
Epoch 29: val_loss did not improve from 0.44559

Epoch 29: ReduceLROnPlateau reducing learning rate to 1e-07.
323/323 [==============================] - 129s 398ms/step - loss: 0.3470 - accuracy: 0.9774 - val_loss: 0.4469 - val_accuracy: 0.9549 - lr: 1.0000e-07
Epoch 30/100
323/323 [==============================] - ETA: 0s - loss: 0.3439 - accuracy: 0.9766
Epoch 30: val_loss did not improve from 0.44559
323/323 [==============================] - 129s 397ms/step - loss: 0.3439 - accuracy: 0.9766 - val_loss: 0.4635 - val_accuracy: 0.9468 - lr: 1.0000e-07
Epoch 31/100
323/323 [==============================] - ETA: 0s - loss: 0.3470 - accuracy: 0.9765
Epoch 31: val_loss did not improve from 0.44559
323/323 [==============================] - 129s 399ms/step - loss: 0.3470 - accuracy: 0.9765 - val_loss: 0.4590 - val_accuracy: 0.9495 - lr: 1.0000e-07
Epoch 32/100
323/323 [==============================] - ETA: 0s - loss: 0.3524 - accuracy: 0.9754
Epoch 32: val_loss did not improve from 0.44559
323/323 [==============================] - 128s 397ms/step - loss: 0.3524 - accuracy: 0.9754 - val_loss: 0.4524 - val_accuracy: 0.9538 - lr: 1.0000e-07
Epoch 33/100
323/323 [==============================] - ETA: 0s - loss: 0.3420 - accuracy: 0.9790
Epoch 33: val_loss did not improve from 0.44559
323/323 [==============================] - 129s 397ms/step - loss: 0.3420 - accuracy: 0.9790 - val_loss: 0.4468 - val_accuracy: 0.9514 - lr: 1.0000e-07
Test Accuracy: 0.9688
173/173 [==============================] - 14s 77ms/step
Classification Report:
              precision    recall  f1-score   support

     battery       0.98      0.98      0.98       390
   cardboard       0.98      0.96      0.97       417
     clothes       1.00      0.99      1.00      1509
       glass       0.98      0.91      0.94       602
       metal       0.90      0.97      0.93       408
     organic       0.97      0.98      0.98       545
       paper       0.94      0.96      0.95       464
     plastic       0.92      0.95      0.94       594
       shoes       0.98      0.99      0.99       592

    accuracy                           0.97      5521
   macro avg       0.96      0.97      0.96      5521
weighted avg       0.97      0.97      0.97      5521

Confusion Matrix: [[ 381    0    0    0    7    0    0    1    1]
 [   2  401    0    0    0    1   11    2    0]
 [   0    0 1496    0    3    0    9    0    1]
 [   2    1    0  548   17    5    0   29    0]
 [   2    1    0    0  396    3    1    4    1]
 [   1    0    0    2    2  533    1    2    4]
 [   0    6    0    0    2    2  445    6    3]
 [   0    0    0   11   12    1    6  563    1]
 [   0    1    0    0    1    2    0    2  586]]

Process finished with exit code 0

27-03-2025-04-04
"C:\Program Files\Python310\python.exe" D:\Study\CNTT\A.MHUD\CNN_Practice\MODEL_EFFICIENTNET.py
GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
TensorFlow is using GPU: True
2025-04-04 06:07:27.551402: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
1 Physical GPU, 1 Logical GPUs
2025-04-04 06:07:28.025226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8192 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6
Found 11792 images belonging to 9 classes.
Found 2944 images belonging to 9 classes.
Found 3680 images belonging to 9 classes.
Training class distribution: Counter({2: 3222, 3: 1285, 7: 1270, 8: 1265, 5: 1165, 6: 992, 1: 890, 4: 871, 0: 832})
Validation class distribution: Counter({2: 805, 3: 321, 7: 317, 8: 316, 5: 291, 6: 247, 1: 222, 4: 217, 0: 208})
Testing class distribution: Counter({2: 1006, 3: 401, 7: 396, 8: 395, 5: 363, 6: 309, 1: 278, 4: 272, 0: 260})
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..255.0].
Total layers: 238, Fine-tuning last 47 layers
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to
==================================================================================================
 input_1 (InputLayer)           [(None, 224, 224, 3  0           []
                                )]

 rescaling (Rescaling)          (None, 224, 224, 3)  0           ['input_1[0][0]']

 normalization (Normalization)  (None, 224, 224, 3)  7           ['rescaling[0][0]']

 tf.math.truediv (TFOpLambda)   (None, 224, 224, 3)  0           ['normalization[0][0]']

 stem_conv_pad (ZeroPadding2D)  (None, 225, 225, 3)  0           ['tf.math.truediv[0][0]']

 stem_conv (Conv2D)             (None, 112, 112, 32  864         ['stem_conv_pad[0][0]']
                                )

 stem_bn (BatchNormalization)   (None, 112, 112, 32  128         ['stem_conv[0][0]']
                                )

 stem_activation (Activation)   (None, 112, 112, 32  0           ['stem_bn[0][0]']
                                )

 block1a_dwconv (DepthwiseConv2  (None, 112, 112, 32  288        ['stem_activation[0][0]']
 D)                             )

 block1a_bn (BatchNormalization  (None, 112, 112, 32  128        ['block1a_dwconv[0][0]']
 )                              )

 block1a_activation (Activation  (None, 112, 112, 32  0          ['block1a_bn[0][0]']
 )                              )

 block1a_se_squeeze (GlobalAver  (None, 32)          0           ['block1a_activation[0][0]']
 agePooling2D)

 block1a_se_reshape (Reshape)   (None, 1, 1, 32)     0           ['block1a_se_squeeze[0][0]']

 block1a_se_reduce (Conv2D)     (None, 1, 1, 8)      264         ['block1a_se_reshape[0][0]']

 block1a_se_expand (Conv2D)     (None, 1, 1, 32)     288         ['block1a_se_reduce[0][0]']

 block1a_se_excite (Multiply)   (None, 112, 112, 32  0           ['block1a_activation[0][0]',
                                )                                 'block1a_se_expand[0][0]']

 block1a_project_conv (Conv2D)  (None, 112, 112, 16  512         ['block1a_se_excite[0][0]']
                                )

 block1a_project_bn (BatchNorma  (None, 112, 112, 16  64         ['block1a_project_conv[0][0]']
 lization)                      )

 block2a_expand_conv (Conv2D)   (None, 112, 112, 96  1536        ['block1a_project_bn[0][0]']
                                )

 block2a_expand_bn (BatchNormal  (None, 112, 112, 96  384        ['block2a_expand_conv[0][0]']
 ization)                       )

 block2a_expand_activation (Act  (None, 112, 112, 96  0          ['block2a_expand_bn[0][0]']
 ivation)                       )

 block2a_dwconv_pad (ZeroPaddin  (None, 113, 113, 96  0          ['block2a_expand_activation[0][0]
 g2D)                           )                                ']

 block2a_dwconv (DepthwiseConv2  (None, 56, 56, 96)  864         ['block2a_dwconv_pad[0][0]']
 D)

 block2a_bn (BatchNormalization  (None, 56, 56, 96)  384         ['block2a_dwconv[0][0]']
 )

 block2a_activation (Activation  (None, 56, 56, 96)  0           ['block2a_bn[0][0]']
 )

 block2a_se_squeeze (GlobalAver  (None, 96)          0           ['block2a_activation[0][0]']
 agePooling2D)

 block2a_se_reshape (Reshape)   (None, 1, 1, 96)     0           ['block2a_se_squeeze[0][0]']

 block2a_se_reduce (Conv2D)     (None, 1, 1, 4)      388         ['block2a_se_reshape[0][0]']

 block2a_se_expand (Conv2D)     (None, 1, 1, 96)     480         ['block2a_se_reduce[0][0]']

 block2a_se_excite (Multiply)   (None, 56, 56, 96)   0           ['block2a_activation[0][0]',
                                                                  'block2a_se_expand[0][0]']

 block2a_project_conv (Conv2D)  (None, 56, 56, 24)   2304        ['block2a_se_excite[0][0]']

 block2a_project_bn (BatchNorma  (None, 56, 56, 24)  96          ['block2a_project_conv[0][0]']
 lization)

 block2b_expand_conv (Conv2D)   (None, 56, 56, 144)  3456        ['block2a_project_bn[0][0]']

 block2b_expand_bn (BatchNormal  (None, 56, 56, 144)  576        ['block2b_expand_conv[0][0]']
 ization)

 block2b_expand_activation (Act  (None, 56, 56, 144)  0          ['block2b_expand_bn[0][0]']
 ivation)

 block2b_dwconv (DepthwiseConv2  (None, 56, 56, 144)  1296       ['block2b_expand_activation[0][0]
 D)                                                              ']

 block2b_bn (BatchNormalization  (None, 56, 56, 144)  576        ['block2b_dwconv[0][0]']
 )

 block2b_activation (Activation  (None, 56, 56, 144)  0          ['block2b_bn[0][0]']
 )

 block2b_se_squeeze (GlobalAver  (None, 144)         0           ['block2b_activation[0][0]']
 agePooling2D)

 block2b_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block2b_se_squeeze[0][0]']

 block2b_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block2b_se_reshape[0][0]']

 block2b_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block2b_se_reduce[0][0]']

 block2b_se_excite (Multiply)   (None, 56, 56, 144)  0           ['block2b_activation[0][0]',
                                                                  'block2b_se_expand[0][0]']

 block2b_project_conv (Conv2D)  (None, 56, 56, 24)   3456        ['block2b_se_excite[0][0]']

 block2b_project_bn (BatchNorma  (None, 56, 56, 24)  96          ['block2b_project_conv[0][0]']
 lization)

 block2b_drop (Dropout)         (None, 56, 56, 24)   0           ['block2b_project_bn[0][0]']

 block2b_add (Add)              (None, 56, 56, 24)   0           ['block2b_drop[0][0]',
                                                                  'block2a_project_bn[0][0]']

 block3a_expand_conv (Conv2D)   (None, 56, 56, 144)  3456        ['block2b_add[0][0]']

 block3a_expand_bn (BatchNormal  (None, 56, 56, 144)  576        ['block3a_expand_conv[0][0]']
 ization)

 block3a_expand_activation (Act  (None, 56, 56, 144)  0          ['block3a_expand_bn[0][0]']
 ivation)

 block3a_dwconv_pad (ZeroPaddin  (None, 59, 59, 144)  0          ['block3a_expand_activation[0][0]
 g2D)                                                            ']

 block3a_dwconv (DepthwiseConv2  (None, 28, 28, 144)  3600       ['block3a_dwconv_pad[0][0]']
 D)

 block3a_bn (BatchNormalization  (None, 28, 28, 144)  576        ['block3a_dwconv[0][0]']
 )

 block3a_activation (Activation  (None, 28, 28, 144)  0          ['block3a_bn[0][0]']
 )

 block3a_se_squeeze (GlobalAver  (None, 144)         0           ['block3a_activation[0][0]']
 agePooling2D)

 block3a_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block3a_se_squeeze[0][0]']

 block3a_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block3a_se_reshape[0][0]']

 block3a_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block3a_se_reduce[0][0]']

 block3a_se_excite (Multiply)   (None, 28, 28, 144)  0           ['block3a_activation[0][0]',
                                                                  'block3a_se_expand[0][0]']

 block3a_project_conv (Conv2D)  (None, 28, 28, 40)   5760        ['block3a_se_excite[0][0]']

 block3a_project_bn (BatchNorma  (None, 28, 28, 40)  160         ['block3a_project_conv[0][0]']
 lization)

 block3b_expand_conv (Conv2D)   (None, 28, 28, 240)  9600        ['block3a_project_bn[0][0]']

 block3b_expand_bn (BatchNormal  (None, 28, 28, 240)  960        ['block3b_expand_conv[0][0]']
 ization)

 block3b_expand_activation (Act  (None, 28, 28, 240)  0          ['block3b_expand_bn[0][0]']
 ivation)

 block3b_dwconv (DepthwiseConv2  (None, 28, 28, 240)  6000       ['block3b_expand_activation[0][0]
 D)                                                              ']

 block3b_bn (BatchNormalization  (None, 28, 28, 240)  960        ['block3b_dwconv[0][0]']
 )

 block3b_activation (Activation  (None, 28, 28, 240)  0          ['block3b_bn[0][0]']
 )

 block3b_se_squeeze (GlobalAver  (None, 240)         0           ['block3b_activation[0][0]']
 agePooling2D)

 block3b_se_reshape (Reshape)   (None, 1, 1, 240)    0           ['block3b_se_squeeze[0][0]']

 block3b_se_reduce (Conv2D)     (None, 1, 1, 10)     2410        ['block3b_se_reshape[0][0]']

 block3b_se_expand (Conv2D)     (None, 1, 1, 240)    2640        ['block3b_se_reduce[0][0]']

 block3b_se_excite (Multiply)   (None, 28, 28, 240)  0           ['block3b_activation[0][0]',
                                                                  'block3b_se_expand[0][0]']

 block3b_project_conv (Conv2D)  (None, 28, 28, 40)   9600        ['block3b_se_excite[0][0]']

 block3b_project_bn (BatchNorma  (None, 28, 28, 40)  160         ['block3b_project_conv[0][0]']
 lization)

 block3b_drop (Dropout)         (None, 28, 28, 40)   0           ['block3b_project_bn[0][0]']

 block3b_add (Add)              (None, 28, 28, 40)   0           ['block3b_drop[0][0]',
                                                                  'block3a_project_bn[0][0]']

 block4a_expand_conv (Conv2D)   (None, 28, 28, 240)  9600        ['block3b_add[0][0]']

 block4a_expand_bn (BatchNormal  (None, 28, 28, 240)  960        ['block4a_expand_conv[0][0]']
 ization)

 block4a_expand_activation (Act  (None, 28, 28, 240)  0          ['block4a_expand_bn[0][0]']
 ivation)

 block4a_dwconv_pad (ZeroPaddin  (None, 29, 29, 240)  0          ['block4a_expand_activation[0][0]
 g2D)                                                            ']

 block4a_dwconv (DepthwiseConv2  (None, 14, 14, 240)  2160       ['block4a_dwconv_pad[0][0]']
 D)

 block4a_bn (BatchNormalization  (None, 14, 14, 240)  960        ['block4a_dwconv[0][0]']
 )

 block4a_activation (Activation  (None, 14, 14, 240)  0          ['block4a_bn[0][0]']
 )

 block4a_se_squeeze (GlobalAver  (None, 240)         0           ['block4a_activation[0][0]']
 agePooling2D)

 block4a_se_reshape (Reshape)   (None, 1, 1, 240)    0           ['block4a_se_squeeze[0][0]']

 block4a_se_reduce (Conv2D)     (None, 1, 1, 10)     2410        ['block4a_se_reshape[0][0]']

 block4a_se_expand (Conv2D)     (None, 1, 1, 240)    2640        ['block4a_se_reduce[0][0]']

 block4a_se_excite (Multiply)   (None, 14, 14, 240)  0           ['block4a_activation[0][0]',
                                                                  'block4a_se_expand[0][0]']

 block4a_project_conv (Conv2D)  (None, 14, 14, 80)   19200       ['block4a_se_excite[0][0]']

 block4a_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4a_project_conv[0][0]']
 lization)

 block4b_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4a_project_bn[0][0]']

 block4b_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block4b_expand_conv[0][0]']
 ization)

 block4b_expand_activation (Act  (None, 14, 14, 480)  0          ['block4b_expand_bn[0][0]']
 ivation)

 block4b_dwconv (DepthwiseConv2  (None, 14, 14, 480)  4320       ['block4b_expand_activation[0][0]
 D)                                                              ']

 block4b_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block4b_dwconv[0][0]']
 )

 block4b_activation (Activation  (None, 14, 14, 480)  0          ['block4b_bn[0][0]']
 )

 block4b_se_squeeze (GlobalAver  (None, 480)         0           ['block4b_activation[0][0]']
 agePooling2D)

 block4b_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block4b_se_squeeze[0][0]']

 block4b_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block4b_se_reshape[0][0]']

 block4b_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block4b_se_reduce[0][0]']

 block4b_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block4b_activation[0][0]',
                                                                  'block4b_se_expand[0][0]']

 block4b_project_conv (Conv2D)  (None, 14, 14, 80)   38400       ['block4b_se_excite[0][0]']

 block4b_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4b_project_conv[0][0]']
 lization)

 block4b_drop (Dropout)         (None, 14, 14, 80)   0           ['block4b_project_bn[0][0]']

 block4b_add (Add)              (None, 14, 14, 80)   0           ['block4b_drop[0][0]',
                                                                  'block4a_project_bn[0][0]']

 block4c_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4b_add[0][0]']

 block4c_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block4c_expand_conv[0][0]']
 ization)

 block4c_expand_activation (Act  (None, 14, 14, 480)  0          ['block4c_expand_bn[0][0]']
 ivation)

 block4c_dwconv (DepthwiseConv2  (None, 14, 14, 480)  4320       ['block4c_expand_activation[0][0]
 D)                                                              ']

 block4c_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block4c_dwconv[0][0]']
 )

 block4c_activation (Activation  (None, 14, 14, 480)  0          ['block4c_bn[0][0]']
 )

 block4c_se_squeeze (GlobalAver  (None, 480)         0           ['block4c_activation[0][0]']
 agePooling2D)

 block4c_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block4c_se_squeeze[0][0]']

 block4c_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block4c_se_reshape[0][0]']

 block4c_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block4c_se_reduce[0][0]']

 block4c_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block4c_activation[0][0]',
                                                                  'block4c_se_expand[0][0]']

 block4c_project_conv (Conv2D)  (None, 14, 14, 80)   38400       ['block4c_se_excite[0][0]']

 block4c_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4c_project_conv[0][0]']
 lization)

 block4c_drop (Dropout)         (None, 14, 14, 80)   0           ['block4c_project_bn[0][0]']

 block4c_add (Add)              (None, 14, 14, 80)   0           ['block4c_drop[0][0]',
                                                                  'block4b_add[0][0]']

 block5a_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4c_add[0][0]']

 block5a_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block5a_expand_conv[0][0]']
 ization)

 block5a_expand_activation (Act  (None, 14, 14, 480)  0          ['block5a_expand_bn[0][0]']
 ivation)

 block5a_dwconv (DepthwiseConv2  (None, 14, 14, 480)  12000      ['block5a_expand_activation[0][0]
 D)                                                              ']

 block5a_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block5a_dwconv[0][0]']
 )

 block5a_activation (Activation  (None, 14, 14, 480)  0          ['block5a_bn[0][0]']
 )

 block5a_se_squeeze (GlobalAver  (None, 480)         0           ['block5a_activation[0][0]']
 agePooling2D)

 block5a_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block5a_se_squeeze[0][0]']

 block5a_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block5a_se_reshape[0][0]']

 block5a_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block5a_se_reduce[0][0]']

 block5a_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block5a_activation[0][0]',
                                                                  'block5a_se_expand[0][0]']

 block5a_project_conv (Conv2D)  (None, 14, 14, 112)  53760       ['block5a_se_excite[0][0]']

 block5a_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5a_project_conv[0][0]']
 lization)

 block5b_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5a_project_bn[0][0]']

 block5b_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block5b_expand_conv[0][0]']
 ization)

 block5b_expand_activation (Act  (None, 14, 14, 672)  0          ['block5b_expand_bn[0][0]']
 ivation)

 block5b_dwconv (DepthwiseConv2  (None, 14, 14, 672)  16800      ['block5b_expand_activation[0][0]
 D)                                                              ']

 block5b_bn (BatchNormalization  (None, 14, 14, 672)  2688       ['block5b_dwconv[0][0]']
 )

 block5b_activation (Activation  (None, 14, 14, 672)  0          ['block5b_bn[0][0]']
 )

 block5b_se_squeeze (GlobalAver  (None, 672)         0           ['block5b_activation[0][0]']
 agePooling2D)

 block5b_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5b_se_squeeze[0][0]']

 block5b_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5b_se_reshape[0][0]']

 block5b_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5b_se_reduce[0][0]']

 block5b_se_excite (Multiply)   (None, 14, 14, 672)  0           ['block5b_activation[0][0]',
                                                                  'block5b_se_expand[0][0]']

 block5b_project_conv (Conv2D)  (None, 14, 14, 112)  75264       ['block5b_se_excite[0][0]']

 block5b_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5b_project_conv[0][0]']
 lization)

 block5b_drop (Dropout)         (None, 14, 14, 112)  0           ['block5b_project_bn[0][0]']

 block5b_add (Add)              (None, 14, 14, 112)  0           ['block5b_drop[0][0]',
                                                                  'block5a_project_bn[0][0]']

 block5c_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5b_add[0][0]']

 block5c_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block5c_expand_conv[0][0]']
 ization)

 block5c_expand_activation (Act  (None, 14, 14, 672)  0          ['block5c_expand_bn[0][0]']
 ivation)

 block5c_dwconv (DepthwiseConv2  (None, 14, 14, 672)  16800      ['block5c_expand_activation[0][0]
 D)                                                              ']

 block5c_bn (BatchNormalization  (None, 14, 14, 672)  2688       ['block5c_dwconv[0][0]']
 )

 block5c_activation (Activation  (None, 14, 14, 672)  0          ['block5c_bn[0][0]']
 )

 block5c_se_squeeze (GlobalAver  (None, 672)         0           ['block5c_activation[0][0]']
 agePooling2D)

 block5c_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5c_se_squeeze[0][0]']

 block5c_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5c_se_reshape[0][0]']

 block5c_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5c_se_reduce[0][0]']

 block5c_se_excite (Multiply)   (None, 14, 14, 672)  0           ['block5c_activation[0][0]',
                                                                  'block5c_se_expand[0][0]']

 block5c_project_conv (Conv2D)  (None, 14, 14, 112)  75264       ['block5c_se_excite[0][0]']

 block5c_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5c_project_conv[0][0]']
 lization)

 block5c_drop (Dropout)         (None, 14, 14, 112)  0           ['block5c_project_bn[0][0]']

 block5c_add (Add)              (None, 14, 14, 112)  0           ['block5c_drop[0][0]',
                                                                  'block5b_add[0][0]']

 block6a_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5c_add[0][0]']

 block6a_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block6a_expand_conv[0][0]']
 ization)

 block6a_expand_activation (Act  (None, 14, 14, 672)  0          ['block6a_expand_bn[0][0]']
 ivation)

 block6a_dwconv_pad (ZeroPaddin  (None, 17, 17, 672)  0          ['block6a_expand_activation[0][0]
 g2D)                                                            ']

 block6a_dwconv (DepthwiseConv2  (None, 7, 7, 672)   16800       ['block6a_dwconv_pad[0][0]']
 D)

 block6a_bn (BatchNormalization  (None, 7, 7, 672)   2688        ['block6a_dwconv[0][0]']
 )

 block6a_activation (Activation  (None, 7, 7, 672)   0           ['block6a_bn[0][0]']
 )

 block6a_se_squeeze (GlobalAver  (None, 672)         0           ['block6a_activation[0][0]']
 agePooling2D)

 block6a_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block6a_se_squeeze[0][0]']

 block6a_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block6a_se_reshape[0][0]']

 block6a_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block6a_se_reduce[0][0]']

 block6a_se_excite (Multiply)   (None, 7, 7, 672)    0           ['block6a_activation[0][0]',
                                                                  'block6a_se_expand[0][0]']

 block6a_project_conv (Conv2D)  (None, 7, 7, 192)    129024      ['block6a_se_excite[0][0]']

 block6a_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6a_project_conv[0][0]']
 lization)

 block6b_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6a_project_bn[0][0]']

 block6b_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6b_expand_conv[0][0]']
 ization)

 block6b_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6b_expand_bn[0][0]']
 ivation)

 block6b_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6b_expand_activation[0][0]
 D)                                                              ']

 block6b_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6b_dwconv[0][0]']
 )

 block6b_activation (Activation  (None, 7, 7, 1152)  0           ['block6b_bn[0][0]']
 )

 block6b_se_squeeze (GlobalAver  (None, 1152)        0           ['block6b_activation[0][0]']
 agePooling2D)

 block6b_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6b_se_squeeze[0][0]']

 block6b_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6b_se_reshape[0][0]']

 block6b_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6b_se_reduce[0][0]']

 block6b_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6b_activation[0][0]',
                                                                  'block6b_se_expand[0][0]']

 block6b_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6b_se_excite[0][0]']

 block6b_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6b_project_conv[0][0]']
 lization)

 block6b_drop (Dropout)         (None, 7, 7, 192)    0           ['block6b_project_bn[0][0]']

 block6b_add (Add)              (None, 7, 7, 192)    0           ['block6b_drop[0][0]',
                                                                  'block6a_project_bn[0][0]']

 block6c_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6b_add[0][0]']

 block6c_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6c_expand_conv[0][0]']
 ization)

 block6c_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6c_expand_bn[0][0]']
 ivation)

 block6c_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6c_expand_activation[0][0]
 D)                                                              ']

 block6c_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6c_dwconv[0][0]']
 )

 block6c_activation (Activation  (None, 7, 7, 1152)  0           ['block6c_bn[0][0]']
 )

 block6c_se_squeeze (GlobalAver  (None, 1152)        0           ['block6c_activation[0][0]']
 agePooling2D)

 block6c_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6c_se_squeeze[0][0]']

 block6c_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6c_se_reshape[0][0]']

 block6c_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6c_se_reduce[0][0]']

 block6c_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6c_activation[0][0]',
                                                                  'block6c_se_expand[0][0]']

 block6c_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6c_se_excite[0][0]']

 block6c_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6c_project_conv[0][0]']
 lization)

 block6c_drop (Dropout)         (None, 7, 7, 192)    0           ['block6c_project_bn[0][0]']

 block6c_add (Add)              (None, 7, 7, 192)    0           ['block6c_drop[0][0]',
                                                                  'block6b_add[0][0]']

 block6d_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6c_add[0][0]']

 block6d_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6d_expand_conv[0][0]']
 ization)

 block6d_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6d_expand_bn[0][0]']
 ivation)

 block6d_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6d_expand_activation[0][0]
 D)                                                              ']

 block6d_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6d_dwconv[0][0]']
 )

 block6d_activation (Activation  (None, 7, 7, 1152)  0           ['block6d_bn[0][0]']
 )

 block6d_se_squeeze (GlobalAver  (None, 1152)        0           ['block6d_activation[0][0]']
 agePooling2D)

 block6d_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6d_se_squeeze[0][0]']

 block6d_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6d_se_reshape[0][0]']

 block6d_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6d_se_reduce[0][0]']

 block6d_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6d_activation[0][0]',
                                                                  'block6d_se_expand[0][0]']

 block6d_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6d_se_excite[0][0]']

 block6d_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6d_project_conv[0][0]']
 lization)

 block6d_drop (Dropout)         (None, 7, 7, 192)    0           ['block6d_project_bn[0][0]']

 block6d_add (Add)              (None, 7, 7, 192)    0           ['block6d_drop[0][0]',
                                                                  'block6c_add[0][0]']

 block7a_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6d_add[0][0]']

 block7a_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block7a_expand_conv[0][0]']
 ization)

 block7a_expand_activation (Act  (None, 7, 7, 1152)  0           ['block7a_expand_bn[0][0]']
 ivation)

 block7a_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  10368       ['block7a_expand_activation[0][0]
 D)                                                              ']

 block7a_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block7a_dwconv[0][0]']
 )

 block7a_activation (Activation  (None, 7, 7, 1152)  0           ['block7a_bn[0][0]']
 )

 block7a_se_squeeze (GlobalAver  (None, 1152)        0           ['block7a_activation[0][0]']
 agePooling2D)

 block7a_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block7a_se_squeeze[0][0]']

 block7a_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block7a_se_reshape[0][0]']

 block7a_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block7a_se_reduce[0][0]']

 block7a_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block7a_activation[0][0]',
                                                                  'block7a_se_expand[0][0]']

 block7a_project_conv (Conv2D)  (None, 7, 7, 320)    368640      ['block7a_se_excite[0][0]']

 block7a_project_bn (BatchNorma  (None, 7, 7, 320)   1280        ['block7a_project_conv[0][0]']
 lization)

 top_conv (Conv2D)              (None, 7, 7, 1280)   409600      ['block7a_project_bn[0][0]']

 top_bn (BatchNormalization)    (None, 7, 7, 1280)   5120        ['top_conv[0][0]']

 top_activation (Activation)    (None, 7, 7, 1280)   0           ['top_bn[0][0]']

 conv2d (Conv2D)                (None, 7, 7, 128)    1474688     ['top_activation[0][0]']

 max_pooling2d (MaxPooling2D)   (None, 3, 3, 128)    0           ['conv2d[0][0]']

 batch_normalization (BatchNorm  (None, 3, 3, 128)   512         ['max_pooling2d[0][0]']
 alization)

 conv2d_1 (Conv2D)              (None, 3, 3, 128)    147584      ['batch_normalization[0][0]']

 max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 128)   0           ['conv2d_1[0][0]']

 batch_normalization_1 (BatchNo  (None, 1, 1, 128)   512         ['max_pooling2d_1[0][0]']
 rmalization)

 flatten (Flatten)              (None, 128)          0           ['batch_normalization_1[0][0]']

 dense (Dense)                  (None, 512)          66048       ['flatten[0][0]']

 batch_normalization_2 (BatchNo  (None, 512)         2048        ['dense[0][0]']
 rmalization)

 dropout (Dropout)              (None, 512)          0           ['batch_normalization_2[0][0]']

 dense_1 (Dense)                (None, 256)          131328      ['dropout[0][0]']

 batch_normalization_3 (BatchNo  (None, 256)         1024        ['dense_1[0][0]']
 rmalization)

 dropout_1 (Dropout)            (None, 256)          0           ['batch_normalization_3[0][0]']

 dense_2 (Dense)                (None, 128)          32896       ['dropout_1[0][0]']

 batch_normalization_4 (BatchNo  (None, 128)         512         ['dense_2[0][0]']
 rmalization)

 dropout_2 (Dropout)            (None, 128)          0           ['batch_normalization_4[0][0]']

 dense_3 (Dense)                (None, 9)            1161        ['dropout_2[0][0]']

==================================================================================================
Total params: 5,907,884
Trainable params: 5,863,557
Non-trainable params: 44,327
__________________________________________________________________________________________________
Epoch 1/100
2025-04-04 06:07:40.914030: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8907
2025-04-04 06:07:42.210708: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
 16/369 [>.............................] - ETA: 1:53 - loss: 3.4591 - accuracy: 0.2012C:\Program Files\Python310\lib\site-packages\PIL\Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
369/369 [==============================] - ETA: 0s - loss: 1.7329 - accuracy: 0.7003
Epoch 1: val_loss improved from inf to 1.25243, saving model to efficientnet_saved_2025-04-04_06-07-28.keras
369/369 [==============================] - 156s 397ms/step - loss: 1.7329 - accuracy: 0.7003 - val_loss: 1.2524 - val_accuracy: 0.8329 - lr: 0.0010
Epoch 2/100
369/369 [==============================] - ETA: 0s - loss: 1.1680 - accuracy: 0.8562
Epoch 2: val_loss improved from 1.25243 to 1.00188, saving model to efficientnet_saved_2025-04-04_06-07-28.keras
369/369 [==============================] - 146s 396ms/step - loss: 1.1680 - accuracy: 0.8562 - val_loss: 1.0019 - val_accuracy: 0.8787 - lr: 0.0010
Epoch 3/100
369/369 [==============================] - ETA: 0s - loss: 0.9915 - accuracy: 0.8792
Epoch 3: val_loss did not improve from 1.00188
369/369 [==============================] - 146s 395ms/step - loss: 0.9915 - accuracy: 0.8792 - val_loss: 1.0231 - val_accuracy: 0.8393 - lr: 0.0010
Epoch 4/100
369/369 [==============================] - ETA: 0s - loss: 0.8312 - accuracy: 0.8964
Epoch 4: val_loss improved from 1.00188 to 0.88221, saving model to efficientnet_saved_2025-04-04_06-07-28.keras
369/369 [==============================] - 147s 396ms/step - loss: 0.8312 - accuracy: 0.8964 - val_loss: 0.8822 - val_accuracy: 0.8587 - lr: 0.0010
Epoch 5/100
369/369 [==============================] - ETA: 0s - loss: 0.7368 - accuracy: 0.8976
Epoch 5: val_loss improved from 0.88221 to 0.67042, saving model to efficientnet_saved_2025-04-04_06-07-28.keras
369/369 [==============================] - 147s 398ms/step - loss: 0.7368 - accuracy: 0.8976 - val_loss: 0.6704 - val_accuracy: 0.9001 - lr: 0.0010
Epoch 6/100
369/369 [==============================] - ETA: 0s - loss: 0.6413 - accuracy: 0.9060
Epoch 6: val_loss improved from 0.67042 to 0.63347, saving model to efficientnet_saved_2025-04-04_06-07-28.keras
369/369 [==============================] - 146s 395ms/step - loss: 0.6413 - accuracy: 0.9060 - val_loss: 0.6335 - val_accuracy: 0.8903 - lr: 0.0010
Epoch 7/100
369/369 [==============================] - ETA: 0s - loss: 0.5756 - accuracy: 0.9077
Epoch 7: val_loss did not improve from 0.63347
369/369 [==============================] - 147s 397ms/step - loss: 0.5756 - accuracy: 0.9077 - val_loss: 0.7875 - val_accuracy: 0.8475 - lr: 0.0010
Epoch 8/100
369/369 [==============================] - ETA: 0s - loss: 0.5351 - accuracy: 0.9073
Epoch 8: val_loss improved from 0.63347 to 0.53708, saving model to efficientnet_saved_2025-04-04_06-07-28.keras
369/369 [==============================] - 147s 398ms/step - loss: 0.5351 - accuracy: 0.9073 - val_loss: 0.5371 - val_accuracy: 0.8964 - lr: 0.0010
Epoch 9/100
369/369 [==============================] - ETA: 0s - loss: 0.4893 - accuracy: 0.9082
Epoch 9: val_loss improved from 0.53708 to 0.46518, saving model to efficientnet_saved_2025-04-04_06-07-28.keras
369/369 [==============================] - 147s 397ms/step - loss: 0.4893 - accuracy: 0.9082 - val_loss: 0.4652 - val_accuracy: 0.9001 - lr: 0.0010
Epoch 10/100
369/369 [==============================] - ETA: 0s - loss: 0.4580 - accuracy: 0.9136
Epoch 10: val_loss did not improve from 0.46518
369/369 [==============================] - 147s 397ms/step - loss: 0.4580 - accuracy: 0.9136 - val_loss: 0.5361 - val_accuracy: 0.8849 - lr: 0.0010
Epoch 11/100
369/369 [==============================] - ETA: 0s - loss: 0.4329 - accuracy: 0.9164
Epoch 11: val_loss did not improve from 0.46518
369/369 [==============================] - 147s 397ms/step - loss: 0.4329 - accuracy: 0.9164 - val_loss: 0.5574 - val_accuracy: 0.8689 - lr: 0.0010
Epoch 12/100
369/369 [==============================] - ETA: 0s - loss: 0.4001 - accuracy: 0.9203
Epoch 12: val_loss did not improve from 0.46518

Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
369/369 [==============================] - 147s 397ms/step - loss: 0.4001 - accuracy: 0.9203 - val_loss: 0.6576 - val_accuracy: 0.8471 - lr: 0.0010
Epoch 13/100
369/369 [==============================] - ETA: 0s - loss: 0.2934 - accuracy: 0.9478
Epoch 13: val_loss improved from 0.46518 to 0.27724, saving model to efficientnet_saved_2025-04-04_06-07-28.keras
369/369 [==============================] - 148s 399ms/step - loss: 0.2934 - accuracy: 0.9478 - val_loss: 0.2772 - val_accuracy: 0.9460 - lr: 1.0000e-04
Epoch 14/100
369/369 [==============================] - ETA: 0s - loss: 0.2247 - accuracy: 0.9641
Epoch 14: val_loss improved from 0.27724 to 0.26576, saving model to efficientnet_saved_2025-04-04_06-07-28.keras
369/369 [==============================] - 148s 400ms/step - loss: 0.2247 - accuracy: 0.9641 - val_loss: 0.2658 - val_accuracy: 0.9501 - lr: 1.0000e-04
Epoch 15/100
369/369 [==============================] - ETA: 0s - loss: 0.1845 - accuracy: 0.9739
Epoch 15: val_loss improved from 0.26576 to 0.25418, saving model to efficientnet_saved_2025-04-04_06-07-28.keras
369/369 [==============================] - 148s 399ms/step - loss: 0.1845 - accuracy: 0.9739 - val_loss: 0.2542 - val_accuracy: 0.9524 - lr: 1.0000e-04
Epoch 16/100
369/369 [==============================] - ETA: 0s - loss: 0.1702 - accuracy: 0.9774
Epoch 16: val_loss improved from 0.25418 to 0.25107, saving model to efficientnet_saved_2025-04-04_06-07-28.keras
369/369 [==============================] - 148s 401ms/step - loss: 0.1702 - accuracy: 0.9774 - val_loss: 0.2511 - val_accuracy: 0.9518 - lr: 1.0000e-04
Epoch 17/100
369/369 [==============================] - ETA: 0s - loss: 0.1521 - accuracy: 0.9796
Epoch 17: val_loss did not improve from 0.25107
369/369 [==============================] - 147s 398ms/step - loss: 0.1521 - accuracy: 0.9796 - val_loss: 0.2530 - val_accuracy: 0.9552 - lr: 1.0000e-04
Epoch 18/100
369/369 [==============================] - ETA: 0s - loss: 0.1375 - accuracy: 0.9813
Epoch 18: val_loss improved from 0.25107 to 0.22303, saving model to efficientnet_saved_2025-04-04_06-07-28.keras
369/369 [==============================] - 148s 400ms/step - loss: 0.1375 - accuracy: 0.9813 - val_loss: 0.2230 - val_accuracy: 0.9620 - lr: 1.0000e-04
Epoch 19/100
369/369 [==============================] - ETA: 0s - loss: 0.1275 - accuracy: 0.9821
Epoch 19: val_loss did not improve from 0.22303
369/369 [==============================] - 147s 398ms/step - loss: 0.1275 - accuracy: 0.9821 - val_loss: 0.2328 - val_accuracy: 0.9562 - lr: 1.0000e-04
Epoch 20/100
369/369 [==============================] - ETA: 0s - loss: 0.1081 - accuracy: 0.9862
Epoch 20: val_loss did not improve from 0.22303
369/369 [==============================] - 148s 399ms/step - loss: 0.1081 - accuracy: 0.9862 - val_loss: 0.2448 - val_accuracy: 0.9538 - lr: 1.0000e-04
Epoch 21/100
369/369 [==============================] - ETA: 0s - loss: 0.1096 - accuracy: 0.9851
Epoch 21: val_loss improved from 0.22303 to 0.20791, saving model to efficientnet_saved_2025-04-04_06-07-28.keras
369/369 [==============================] - 148s 400ms/step - loss: 0.1096 - accuracy: 0.9851 - val_loss: 0.2079 - val_accuracy: 0.9606 - lr: 1.0000e-04
Epoch 22/100
369/369 [==============================] - ETA: 0s - loss: 0.1037 - accuracy: 0.9859
Epoch 22: val_loss did not improve from 0.20791
369/369 [==============================] - 147s 398ms/step - loss: 0.1037 - accuracy: 0.9859 - val_loss: 0.2375 - val_accuracy: 0.9538 - lr: 1.0000e-04
Epoch 23/100
369/369 [==============================] - ETA: 0s - loss: 0.0929 - accuracy: 0.9867
Epoch 23: val_loss did not improve from 0.20791
369/369 [==============================] - 148s 399ms/step - loss: 0.0929 - accuracy: 0.9867 - val_loss: 0.2522 - val_accuracy: 0.9521 - lr: 1.0000e-04
Epoch 24/100
369/369 [==============================] - ETA: 0s - loss: 0.0904 - accuracy: 0.9886
Epoch 24: val_loss did not improve from 0.20791

Epoch 24: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
369/369 [==============================] - 148s 399ms/step - loss: 0.0904 - accuracy: 0.9886 - val_loss: 0.2192 - val_accuracy: 0.9565 - lr: 1.0000e-04
Epoch 25/100
369/369 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 0.9904
Epoch 25: val_loss did not improve from 0.20791
369/369 [==============================] - 147s 398ms/step - loss: 0.0791 - accuracy: 0.9904 - val_loss: 0.2239 - val_accuracy: 0.9609 - lr: 1.0000e-05
Epoch 26/100
369/369 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9914
Epoch 26: val_loss did not improve from 0.20791
369/369 [==============================] - 148s 400ms/step - loss: 0.0707 - accuracy: 0.9914 - val_loss: 0.2297 - val_accuracy: 0.9599 - lr: 1.0000e-05
Epoch 27/100
369/369 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9916
Epoch 27: val_loss did not improve from 0.20791

Epoch 27: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
369/369 [==============================] - 147s 398ms/step - loss: 0.0721 - accuracy: 0.9916 - val_loss: 0.2191 - val_accuracy: 0.9603 - lr: 1.0000e-05
Epoch 28/100
369/369 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9913
Epoch 28: val_loss improved from 0.20791 to 0.20270, saving model to efficientnet_saved_2025-04-04_06-07-28.keras
369/369 [==============================] - 148s 401ms/step - loss: 0.0718 - accuracy: 0.9913 - val_loss: 0.2027 - val_accuracy: 0.9609 - lr: 1.0000e-06
Epoch 29/100
369/369 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9908
Epoch 29: val_loss did not improve from 0.20270
369/369 [==============================] - 147s 398ms/step - loss: 0.0712 - accuracy: 0.9908 - val_loss: 0.2047 - val_accuracy: 0.9613 - lr: 1.0000e-06
Epoch 30/100
369/369 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9914
Epoch 30: val_loss did not improve from 0.20270
369/369 [==============================] - 147s 398ms/step - loss: 0.0715 - accuracy: 0.9914 - val_loss: 0.2076 - val_accuracy: 0.9640 - lr: 1.0000e-06
Epoch 31/100
369/369 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9914
Epoch 31: val_loss improved from 0.20270 to 0.20143, saving model to efficientnet_saved_2025-04-04_06-07-28.keras
369/369 [==============================] - 148s 400ms/step - loss: 0.0709 - accuracy: 0.9914 - val_loss: 0.2014 - val_accuracy: 0.9626 - lr: 1.0000e-06
Epoch 32/100
369/369 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9924
Epoch 32: val_loss did not improve from 0.20143
369/369 [==============================] - 147s 399ms/step - loss: 0.0698 - accuracy: 0.9924 - val_loss: 0.2082 - val_accuracy: 0.9637 - lr: 1.0000e-06
Epoch 33/100
369/369 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9924
Epoch 33: val_loss did not improve from 0.20143
369/369 [==============================] - 148s 399ms/step - loss: 0.0719 - accuracy: 0.9924 - val_loss: 0.2088 - val_accuracy: 0.9616 - lr: 1.0000e-06
Epoch 34/100
369/369 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9919
Epoch 34: val_loss did not improve from 0.20143

Epoch 34: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.
369/369 [==============================] - 147s 399ms/step - loss: 0.0713 - accuracy: 0.9919 - val_loss: 0.2041 - val_accuracy: 0.9599 - lr: 1.0000e-06
Epoch 35/100
369/369 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9919
Epoch 35: val_loss did not improve from 0.20143
369/369 [==============================] - 148s 399ms/step - loss: 0.0681 - accuracy: 0.9919 - val_loss: 0.2066 - val_accuracy: 0.9582 - lr: 1.0000e-07
Epoch 36/100
369/369 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9912
Epoch 36: val_loss did not improve from 0.20143
369/369 [==============================] - 150s 405ms/step - loss: 0.0704 - accuracy: 0.9912 - val_loss: 0.2016 - val_accuracy: 0.9643 - lr: 1.0000e-07
Epoch 37/100
369/369 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9925
Epoch 37: val_loss did not improve from 0.20143

Epoch 37: ReduceLROnPlateau reducing learning rate to 1e-07.
369/369 [==============================] - 147s 398ms/step - loss: 0.0695 - accuracy: 0.9925 - val_loss: 0.2187 - val_accuracy: 0.9620 - lr: 1.0000e-07
Epoch 38/100
369/369 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9914
Epoch 38: val_loss improved from 0.20143 to 0.19984, saving model to efficientnet_saved_2025-04-04_06-07-28.keras
369/369 [==============================] - 150s 406ms/step - loss: 0.0676 - accuracy: 0.9914 - val_loss: 0.1998 - val_accuracy: 0.9620 - lr: 1.0000e-07
Epoch 39/100
369/369 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9919
Epoch 39: val_loss did not improve from 0.19984
369/369 [==============================] - 151s 409ms/step - loss: 0.0684 - accuracy: 0.9919 - val_loss: 0.2088 - val_accuracy: 0.9623 - lr: 1.0000e-07
Epoch 40/100
369/369 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9909
Epoch 40: val_loss did not improve from 0.19984
369/369 [==============================] - 149s 402ms/step - loss: 0.0749 - accuracy: 0.9909 - val_loss: 0.2172 - val_accuracy: 0.9596 - lr: 1.0000e-07
Epoch 41/100
369/369 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9922
Epoch 41: val_loss did not improve from 0.19984
369/369 [==============================] - 147s 398ms/step - loss: 0.0691 - accuracy: 0.9922 - val_loss: 0.2030 - val_accuracy: 0.9626 - lr: 1.0000e-07
Epoch 42/100
369/369 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9899
Epoch 42: val_loss improved from 0.19984 to 0.19973, saving model to efficientnet_saved_2025-04-04_06-07-28.keras
369/369 [==============================] - 148s 401ms/step - loss: 0.0738 - accuracy: 0.9899 - val_loss: 0.1997 - val_accuracy: 0.9671 - lr: 1.0000e-07
Epoch 43/100
369/369 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9919
Epoch 43: val_loss did not improve from 0.19973
369/369 [==============================] - 148s 399ms/step - loss: 0.0710 - accuracy: 0.9919 - val_loss: 0.2100 - val_accuracy: 0.9620 - lr: 1.0000e-07
Epoch 44/100
369/369 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9925
Epoch 44: val_loss did not improve from 0.19973
369/369 [==============================] - 148s 399ms/step - loss: 0.0682 - accuracy: 0.9925 - val_loss: 0.2164 - val_accuracy: 0.9603 - lr: 1.0000e-07
Epoch 45/100
369/369 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9909
Epoch 45: val_loss did not improve from 0.19973
369/369 [==============================] - 147s 397ms/step - loss: 0.0730 - accuracy: 0.9909 - val_loss: 0.2222 - val_accuracy: 0.9599 - lr: 1.0000e-07
Epoch 46/100
369/369 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9918
Epoch 46: val_loss did not improve from 0.19973
369/369 [==============================] - 147s 398ms/step - loss: 0.0696 - accuracy: 0.9918 - val_loss: 0.2138 - val_accuracy: 0.9633 - lr: 1.0000e-07
Epoch 47/100
369/369 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9918
Epoch 47: val_loss improved from 0.19973 to 0.19563, saving model to efficientnet_saved_2025-04-04_06-07-28.keras
369/369 [==============================] - 147s 399ms/step - loss: 0.0696 - accuracy: 0.9918 - val_loss: 0.1956 - val_accuracy: 0.9654 - lr: 1.0000e-07
Epoch 48/100
369/369 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9916
Epoch 48: val_loss did not improve from 0.19563
369/369 [==============================] - 147s 397ms/step - loss: 0.0698 - accuracy: 0.9916 - val_loss: 0.2240 - val_accuracy: 0.9603 - lr: 1.0000e-07
Epoch 49/100
369/369 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9932
Epoch 49: val_loss did not improve from 0.19563
369/369 [==============================] - 147s 398ms/step - loss: 0.0657 - accuracy: 0.9932 - val_loss: 0.2188 - val_accuracy: 0.9589 - lr: 1.0000e-07
Epoch 50/100
369/369 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9913
Epoch 50: val_loss did not improve from 0.19563
369/369 [==============================] - 147s 398ms/step - loss: 0.0726 - accuracy: 0.9913 - val_loss: 0.2042 - val_accuracy: 0.9620 - lr: 1.0000e-07
Epoch 51/100
369/369 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9909
Epoch 51: val_loss did not improve from 0.19563
369/369 [==============================] - 148s 400ms/step - loss: 0.0720 - accuracy: 0.9909 - val_loss: 0.2096 - val_accuracy: 0.9657 - lr: 1.0000e-07
Epoch 52/100
369/369 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9903
Epoch 52: val_loss did not improve from 0.19563
369/369 [==============================] - 147s 398ms/step - loss: 0.0730 - accuracy: 0.9903 - val_loss: 0.2257 - val_accuracy: 0.9582 - lr: 1.0000e-07
Epoch 53/100
369/369 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9918
Epoch 53: val_loss did not improve from 0.19563
369/369 [==============================] - 147s 397ms/step - loss: 0.0689 - accuracy: 0.9918 - val_loss: 0.2185 - val_accuracy: 0.9586 - lr: 1.0000e-07
Epoch 54/100
369/369 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9908
Epoch 54: val_loss did not improve from 0.19563
369/369 [==============================] - 147s 397ms/step - loss: 0.0734 - accuracy: 0.9908 - val_loss: 0.2151 - val_accuracy: 0.9609 - lr: 1.0000e-07
Epoch 55/100
369/369 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9911
Epoch 55: val_loss did not improve from 0.19563
369/369 [==============================] - 147s 397ms/step - loss: 0.0731 - accuracy: 0.9911 - val_loss: 0.1966 - val_accuracy: 0.9640 - lr: 1.0000e-07
Epoch 56/100
369/369 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9925
Epoch 56: val_loss did not improve from 0.19563
369/369 [==============================] - 147s 397ms/step - loss: 0.0701 - accuracy: 0.9925 - val_loss: 0.2149 - val_accuracy: 0.9603 - lr: 1.0000e-07
Epoch 57/100
369/369 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9915
Epoch 57: val_loss did not improve from 0.19563
369/369 [==============================] - 147s 397ms/step - loss: 0.0688 - accuracy: 0.9915 - val_loss: 0.2201 - val_accuracy: 0.9620 - lr: 1.0000e-07
Test Accuracy: 0.9698
115/115 [==============================] - 10s 79ms/step
Classification Report:
              precision    recall  f1-score   support

     battery       0.97      0.97      0.97       260
   cardboard       0.98      0.93      0.96       278
     clothes       0.99      1.00      0.99      1006
       glass       0.96      0.93      0.94       401
       metal       0.93      0.96      0.94       272
     organic       0.99      0.99      0.99       363
       paper       0.93      0.97      0.95       309
     plastic       0.94      0.95      0.94       396
       shoes       0.98      0.99      0.99       395

    accuracy                           0.97      3680
   macro avg       0.96      0.96      0.96      3680
weighted avg       0.97      0.97      0.97      3680

Confusion Matrix: [[ 252    2    0    0    2    0    2    2    0]
 [   2  259    2    0    2    0    9    3    1]
 [   0    0 1002    0    0    0    4    0    0]
 [   0    1    0  373   10    3    1   13    0]
 [   4    0    1    2  260    0    1    2    2]
 [   0    0    1    1    0  358    0    0    3]
 [   1    2    0    1    2    0  300    3    0]
 [   1    0    1   12    3    0    4  375    0]
 [   0    0    2    0    1    0    1    1  390]]

Process finished with exit code 0

